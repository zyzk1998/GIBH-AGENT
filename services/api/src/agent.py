import json
import asyncio
from typing import AsyncGenerator, Union
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage
from .config import settings

class BioBlendAgent:
    def __init__(self):
        # è¿æ¥åˆ° vLLM (RTX 6000)
        self.llm = ChatOpenAI(
            model=settings.LLM_MODEL,
            base_url=settings.VLLM_URL,
            api_key="EMPTY",
            temperature=0.1, 
            max_tokens=4096,
            streaming=True
        )

    async def process_query(self, query: str, history: list, uploaded_files: list = None) -> Union[dict, AsyncGenerator]:
        """
        æ™ºèƒ½å¤„ç†å…¥å£
        """
        query_text = query.lower().strip()
        
        # 1. æ˜¾å¼æ„å›¾è¯†åˆ«
        if any(k in query_text for k in ["è§„åˆ’", "æµç¨‹", "workflow", "pipeline"]):
            return self._generate_workflow_config(query, uploaded_files)

        # 2. éšå¼æ„å›¾è¯†åˆ« (Context Awareness)
        if uploaded_files and (not query_text or query_text == "å‘é€äº†æ–‡ä»¶" or len(query_text) < 5):
            if history and len(history) > 0:
                last_msg = history[-1]
                if last_msg.get('role') == 'assistant' and "æœªä¸Šä¼ æ•°æ®" in last_msg.get('content', ''):
                    return self._generate_workflow_config("è§„åˆ’æµç¨‹", uploaded_files)

        # 3. é»˜è®¤ï¼šæµå¼å¯¹è¯ (å¸¦æ·±åº¦æ€è€ƒ)
        return self._stream_chat(query, uploaded_files)

    def _get_filename(self, f):
        if isinstance(f, dict):
            return f.get('name', 'unknown')
        return getattr(f, 'name', 'unknown')

    def _generate_workflow_config(self, query, uploaded_files=None):
        """
        ç”Ÿæˆå‰ç«¯å¯æ¸²æŸ“çš„å·¥ä½œæµé…ç½®å¡ç‰‡
        """
        if not uploaded_files:
            reply_text = "å·²ä¸ºæ‚¨è§„åˆ’æ ‡å‡†åˆ†ææµç¨‹ã€‚âš ï¸ **æ£€æµ‹åˆ°æ‚¨å°šæœªä¸Šä¼ æ•°æ®**ï¼Œè¯·å…ˆç‚¹å‡»å›å½¢é’ˆä¸Šä¼  .h5ad æˆ– .mtx æ–‡ä»¶ï¼Œç„¶åå†ç‚¹å‡»â€œæ‰§è¡Œâ€ã€‚"
        else:
            names = [self._get_filename(f) for f in uploaded_files]
            file_names = ", ".join(names)
            reply_text = f"æ”¶åˆ°æ–‡ä»¶ï¼š**{file_names}**ã€‚\nå·²ä¸ºæ‚¨è‡ªåŠ¨åŒ¹é… **Standard Scanpy Pipeline (10 Steps)**ï¼Œæ¶µç›–ä»è´¨æ§åˆ°å¤šç»´å¯è§†åŒ–çš„å…¨æµç¨‹ã€‚è¯·ç¡®è®¤å‚æ•°ï¼š"

        return {
            "type": "workflow_config",
            "reply": reply_text,
            "workflow_name": "Standard Scanpy Pipeline",
            "steps": [
                {"name": "1. Quality Control", "tool_id": "local_qc", "desc": "Filter cells & genes", "params": [{"name": "min_genes", "label": "Min Genes", "value": "200", "type": "text"}, {"name": "max_mt", "label": "Max MT%", "value": "20", "type": "text"}]},
                {"name": "2. Normalization", "tool_id": "local_normalize", "desc": "Log1p Normalize", "params": []},
                {"name": "3. Find Variable Genes", "tool_id": "local_hvg", "desc": "Select top 2000 genes", "params": []},
                {"name": "4. Scale Data", "tool_id": "local_scale", "desc": "Scale to unit variance", "params": []},
                {"name": "5. PCA", "tool_id": "local_pca", "desc": "Dimensionality Reduction", "params": []},
                {"name": "6. Compute Neighbors", "tool_id": "local_neighbors", "desc": "Build neighborhood graph", "params": []},
                {"name": "7. Clustering", "tool_id": "local_cluster", "desc": "Leiden Clustering", "params": [{"name": "resolution", "label": "Resolution", "value": "0.5", "type": "text"}]},
                {"name": "8. UMAP Visualization", "tool_id": "local_umap", "desc": "Non-linear embedding", "params": []},
                {"name": "9. t-SNE Visualization", "tool_id": "local_tsne", "desc": "t-SNE Visualization", "params": []},
                {"name": "10. Find Markers", "tool_id": "local_markers", "desc": "Identify cluster markers", "params": []}
            ],
            "thought": "è¯†åˆ«åˆ°ç”¨æˆ·éœ€è¦è§„åˆ’åˆ†ææµç¨‹ï¼Œå·²åŠ è½½ Scanpy å®Œæ•´æ ‡å‡†æ¨¡æ¿ã€‚"
        }

    async def _stream_chat(self, query: str, uploaded_files=None) -> AsyncGenerator[str, None]:
        """
        æµå¼å¯¹è¯ç”Ÿæˆå™¨ (æ³¨å…¥æ€ç»´é“¾æŒ‡ä»¤)
        """
        # 1. æ„å»ºä¸Šä¸‹æ–‡
        file_context = ""
        if uploaded_files:
            names = [self._get_filename(f) for f in uploaded_files]
            file_context = f"\n[User Context - Uploaded Files]: {', '.join(names)}"

        # 2. å®šä¹‰ç³»ç»Ÿäººè®¾ (System Prompt) - ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå¼ºåˆ¶è¾“å‡º <think> æ ‡ç­¾
        system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©ä¿¡æ¯å­¦ä¸“å®¶åŠ©æ‰‹ GIBH-Agentã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å›ç­”é—®é¢˜ï¼š
1. **æ·±åº¦æ€è€ƒ (Thinking Process)**ï¼šé¦–å…ˆï¼Œåœ¨ `<think>` å’Œ `</think>` æ ‡ç­¾å†…ï¼Œè¯¦ç»†è§„åˆ’ä½ çš„å›ç­”é€»è¾‘ã€åˆ†æç”¨æˆ·æ„å›¾ã€æ£€æŸ¥æ˜¯å¦æœ‰æ½œåœ¨çš„å‘ï¼ˆå¦‚æ–‡ä»¶æ ¼å¼ã€å‚æ•°è®¾ç½®ï¼‰ã€‚è¿™éƒ¨åˆ†å†…å®¹ä¸è¦ç›´æ¥å±•ç¤ºç»™ç”¨æˆ·çœ‹ã€‚
2. **æ­£å¼å›ç­” (Response)**ï¼šæ€è€ƒç»“æŸåï¼Œåœ¨æ ‡ç­¾å¤–ç»™å‡ºæœ€ç»ˆçš„ã€ç»“æ„æ¸…æ™°çš„å›ç­”ã€‚

è¯·éµå¾ªï¼š
- å‡†ç¡®æ€§ä¼˜å…ˆï¼Œä¸è¦ç¼–é€ ã€‚
- ä½¿ç”¨ Markdown æ ¼å¼ã€‚
- æ‹’ç»æ— å…³é—®é¢˜ã€‚

{file_context}
"""
        system_message = SystemMessagePromptTemplate.from_template(system_template)
        human_message = HumanMessagePromptTemplate.from_template("{query}")
        
        chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])
        
        chain = chat_prompt | self.llm
        
        # 3. æ‰§è¡Œæµå¼ç”Ÿæˆ
        async for chunk in chain.astream({"query": query, "file_context": file_context}):
            content = ""
            if hasattr(chunk, 'content') and chunk.content:
                content = chunk.content
            elif isinstance(chunk, str):
                content = chunk
            
            if content:
                yield content
                # å¹³æ»‘é˜»å°¼ (RTX 6000 ä¸“ç”¨)
                await asyncio.sleep(0.01)
