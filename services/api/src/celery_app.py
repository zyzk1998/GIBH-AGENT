import os
from celery import Celery
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from .config import settings
from .skill_manager import SkillManager 

celery_app = Celery(
    "gibh_worker",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL
)

skill_mgr = SkillManager()

@celery_app.task(bind=True)
def run_bioinformatics_task(self, workflow_data: dict, files: list):
    """
    ç»Ÿä¸€ä»»åŠ¡å…¥å£
    """
    print(f"ğŸš€ [Worker] æ”¶åˆ°ä»»åŠ¡ï¼Œæ–‡ä»¶åˆ—è¡¨: {[f.get('name') for f in files]}")
    return _run_local_skill(self, workflow_data, files)

def _generate_ai_interpretation(qc_metrics, steps_details):
    """
    ğŸ¤– AI Doctor: æ ¹æ®åˆ†æç»“æœç”Ÿæˆä¸“ä¸šè§£è¯»æŠ¥å‘Š
    """
    try:
        # 1. æå–å…³é”®ä¿¡æ¯
        raw_cells = qc_metrics.get('raw_cells', 'N/A')
        filtered_cells = qc_metrics.get('filtered_cells', 'N/A')
        
        # å°è¯•ä»æ­¥éª¤è¯¦æƒ…ä¸­æå– Marker åŸºå› ä¿¡æ¯
        markers_info = "æœªæ‰¾åˆ° Marker åŸºå› ä¿¡æ¯"
        n_clusters = "æœªçŸ¥"
        
        for step in steps_details:
            if step['name'] == 'local_cluster':
                n_clusters = step.get('summary', 'æœªçŸ¥')
            if step['name'] == 'local_markers':
                # è¿™é‡Œé€šå¸¸æ˜¯ HTML è¡¨æ ¼ï¼Œæˆ‘ä»¬ç®€å•æå–ä¸€ä¸‹æˆ–è€…ç›´æ¥æŠŠ HTML æ‰”ç»™ LLM (å¦‚æœä¸å¤ªé•¿)
                # ä¸ºäº†èŠ‚çœ tokenï¼Œè¿™é‡Œå‡è®¾ LLM èƒ½çœ‹æ‡‚ç®€å•çš„ HTML ç»“æ„ï¼Œæˆ–è€…æˆ‘ä»¬åªä¼  summary
                markers_info = step.get('details', 'æœªç”Ÿæˆ Marker è¡¨')

        # 2. è¿æ¥ vLLM (åœ¨ Docker å†…éƒ¨ç½‘ç»œä¸­)
        llm = ChatOpenAI(
            model=settings.LLM_MODEL,
            base_url=settings.VLLM_URL, # http://inference-engine:8000/v1
            api_key="EMPTY",
            temperature=0.2,
            max_tokens=2048
        )

        # 3. æ„é€  Prompt
        prompt_text = f"""
        ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å•ç»†èƒç”Ÿç‰©ä¿¡æ¯å­¦ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ Scanpy åˆ†æç»“æœï¼Œæ’°å†™ä¸€ä»½è¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚

        ã€æ•°æ®æ¦‚å†µã€‘
        - åŸå§‹ç»†èƒæ•°: {raw_cells}
        - è´¨æ§åç»†èƒæ•°: {filtered_cells}
        - èšç±»ç»“æœ: {n_clusters}

        ã€å·®å¼‚åŸºå›  (Markers) æ•°æ®ç‰‡æ®µã€‘
        {markers_info}

        ã€ä»»åŠ¡è¦æ±‚ã€‘
        1. **æ•°æ®è´¨é‡è¯„ä¼°**ï¼šæ ¹æ®è¿‡æ»¤å‰åçš„ç»†èƒæ•°é‡å˜åŒ–ï¼Œè¯„ä»·æ•°æ®è´¨é‡ï¼ˆå¦‚ï¼šæŸå¤±ç‡æ˜¯å¦è¿‡é«˜ï¼Ÿï¼‰ã€‚
        2. **èšç±»åˆ†æ**ï¼šè¯„ä»·èšç±»æ•°é‡æ˜¯å¦åˆç†ã€‚
        3. **ç”Ÿç‰©å­¦æ¨æ–­**ï¼šæ ¹æ® Marker åŸºå› åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå°è¯•æ¨æ–­å¯èƒ½å­˜åœ¨çš„ç»†èƒç±»å‹ï¼ˆå¦‚ Tç»†èƒã€Bç»†èƒç­‰ï¼‰ï¼Œæˆ–è€…æŒ‡å‡ºæœ€æ˜¾è‘—çš„åŸºå› ã€‚
        4. **ä¸‹ä¸€æ­¥å»ºè®®**ï¼šç»™å‡ºåç»­åˆ†æå»ºè®®ï¼ˆå¦‚ç»†èƒæ³¨é‡Šã€æ‹Ÿæ—¶åºåˆ†æï¼‰ã€‚

        è¯·ä½¿ç”¨ Markdown æ ¼å¼è¾“å‡ºï¼Œè¯­æ°”ä¸“ä¸šã€å®¢è§‚ã€‚ä¸è¦è¾“å‡ºä»£ç ï¼Œåªè¾“å‡ºåˆ†ææ–‡æœ¬ã€‚
        """

        prompt = ChatPromptTemplate.from_template(prompt_text)
        chain = prompt | llm
        
        print("ğŸ§  [Worker] æ­£åœ¨è¯·æ±‚ AI ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š...")
        response = chain.invoke({})
        return response.content

    except Exception as e:
        print(f"âš ï¸ [Worker] AI æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return f"ï¼ˆAI è§£è¯»ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨ç†å¼•æ“è¿æ¥ã€‚é”™è¯¯ä¿¡æ¯: {str(e)}ï¼‰\n\nåŸå§‹æ•°æ®æŒ‡æ ‡ï¼šåŸå§‹ç»†èƒ {raw_cells} -> è¿‡æ»¤å {filtered_cells}"

def _run_local_skill(task_instance, workflow_data, files):
    """æ‰§è¡Œæœ¬åœ° Python æ’ä»¶"""
    
    if not files:
        return {"status": "failed", "error": "âŒ é”™è¯¯ï¼šæœªæ¥æ”¶åˆ°æ–‡ä»¶ä¿¡æ¯ã€‚"}
    
    # æ™ºèƒ½è·¯å¾„å¤„ç†
    target_file_name = files[0]['name']
    is_10x = False
    for f in files:
        if 'matrix.mtx' in f['name']:
            is_10x = True
            break
    
    if is_10x:
        data_input_path = settings.UPLOAD_DIR
    else:
        data_input_path = os.path.join(settings.UPLOAD_DIR, target_file_name)

    if not os.path.exists(data_input_path):
        return {"status": "failed", "error": f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è·¯å¾„ {data_input_path}"}
    
    merged_params = {}
    for step in workflow_data['steps']:
        merged_params.update(step.get('params', {}))
    
    skill = skill_mgr.get_skill("scanpy_local")
    if not skill:
        skill_mgr._load_skills()
        skill = skill_mgr.get_skill("scanpy_local")
        if not skill:
            return {"status": "failed", "error": "âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•åŠ è½½ scanpy_local æ’ä»¶ã€‚"}
    
    try:
        print("â–¶ï¸ å¼€å§‹æ‰§è¡Œ Scanpy Pipeline...")
        task_instance.update_state(state='PROGRESS', meta={'steps': [{"name": "æ­£åœ¨åˆå§‹åŒ– Scanpy...", "status": "running"}]})
        
        # 1. æ‰§è¡Œç”Ÿä¿¡åˆ†æ
        result = skill.execute(data_input_path, merged_params, settings.UPLOAD_DIR)
        
        if result['status'] == 'success':
            # 2. ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šè°ƒç”¨ LLM ç”ŸæˆçœŸæ­£çš„è¯Šæ–­æŠ¥å‘Š
            # ç”¨ AI ç”Ÿæˆçš„å†…å®¹è¦†ç›–åŸæœ¬ scrna_analysis.py é‡Œç¡¬ç¼–ç çš„ diagnosis
            task_instance.update_state(state='PROGRESS', meta={'steps': [{"name": "æ­£åœ¨ç”Ÿæˆ AI è¯Šæ–­æŠ¥å‘Š...", "status": "running"}]})
            
            ai_diagnosis = _generate_ai_interpretation(result['qc_metrics'], result['steps_details'])
            result['diagnosis'] = ai_diagnosis
            
        print(f"âœ… æ‰§è¡Œç»“æŸï¼ŒçŠ¶æ€: {result.get('status')}")
        return result
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"status": "failed", "error": f"è¿è¡Œå¼‚å¸¸: {str(e)}"}

def _run_galaxy_task(task_instance, workflow_data, files):
    return {"status": "success", "steps": []}
