import asyncio
import httpx
import time
import json
import random
import string
import pandas as pd
import numpy as np
from colorama import Fore, Style, init
from datetime import datetime

# åˆå§‹åŒ–é¢œè‰²
init(autoreset=True)

# ================= âš™ï¸ å‹æµ‹é…ç½®åŒºåŸŸ (æ ¹æ®éœ€æ±‚è°ƒæ•´) =================
# 1. ç›®æ ‡åœ°å€
AGENT_URL = "http://localhost:8088/api/chat"
JUDGE_URL = "http://localhost:8000/v1/chat/completions"
JUDGE_MODEL = "qwen3-vl"

# 2. å‹åŠ›å‚æ•°
TEST_DURATION_SEC = 120   # â±ï¸ æµ‹è¯•æŒç»­æ—¶é—´ (ç§’)ï¼Œå»ºè®®è®¾ä¸º 300 (5åˆ†é’Ÿ)
CONCURRENCY = 30          # ğŸš€ å¹¶å‘æ•° (RTX 6000 å»ºè®® 30-50ï¼Œå¤ªé«˜ä¼šå¢åŠ å»¶è¿Ÿ)
SAMPLE_RATE = 0.1         # ğŸ” è¯„åˆ†æŠ½æ ·ç‡ (10% çš„å›ç­”ä¼šè¢« AI Doctor æ£€æŸ¥)

# 3. é¢˜åº“ (åŸºç¡€é¢˜ + éšæœºå™ªå£° = æ— é™é¢˜åº“)
BASE_QUESTIONS = [
    "è§£é‡Šå•ç»†èƒæµ‹åºä¸­ Batch Effect çš„åŸç†åŠå»é™¤æ–¹æ³•ã€‚",
    "å¦‚ä½•ä½¿ç”¨ Scanpy è¿›è¡Œç»†èƒèšç±»ï¼Ÿè¯·ç»™å‡ºä»£ç ç¤ºä¾‹ã€‚",
    "TP53 åŸºå› åœ¨è‚¿ç˜¤å‘ç”Ÿä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
    "ä»€ä¹ˆæ˜¯ UMAPï¼Ÿå®ƒå’Œ t-SNE æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
    "çº¿ç²’ä½“åŸºå› å«é‡è¿‡é«˜è¯´æ˜äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
    "è¯·è§£é‡Š Seurat æµç¨‹ä¸­çš„ Normalization æ­¥éª¤ã€‚",
    "ä»€ä¹ˆæ˜¯é«˜å˜åŸºå›  (HVG)ï¼Ÿä¸ºä»€ä¹ˆè¦ç­›é€‰å®ƒä»¬ï¼Ÿ",
    "ç»†èƒå‘¨æœŸè¯„åˆ† (Cell Cycle Scoring) æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ",
    "å¦‚ä½•é€šè¿‡æ ‡è®°åŸºå›  (Marker Genes) æ³¨é‡Šç»†èƒç±»å‹ï¼Ÿ",
    "ç®€è¿° scRNA-seq æ•°æ®åˆ†æçš„æ ‡å‡†æµç¨‹ã€‚"
]
# ===============================================================

class StressStats:
    def __init__(self):
        self.total_requests = 0
        self.success_count = 0
        self.error_count = 0
        self.latencies = []     # æ€»è€—æ—¶
        self.ttfts = []         # é¦–å­—å»¶è¿Ÿ
        self.scores = []        # è´¨é‡è¯„åˆ†
        self.start_time = 0
        self.is_running = True

stats = StressStats()

def generate_random_question():
    """ç”Ÿæˆå¸¦éšæœºå™ªå£°çš„é—®é¢˜ï¼Œé˜²æ­¢ç¼“å­˜ä½œå¼Š"""
    base_q = random.choice(BASE_QUESTIONS)
    # æ·»åŠ éšæœºåç¼€ï¼Œå¼ºåˆ¶ LLM é‡æ–°è®¡ç®— Attention
    salt = ''.join(random.choices(string.ascii_letters + string.digits, k=8))
    return f"{base_q} (Ref: {salt})"

async def ai_doctor_grade(client, question, answer):
    """AI åŒ»ç”ŸæŠ½æ£€è¯„åˆ†"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä½ç”Ÿä¿¡ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹å›ç­”æ‰“åˆ†ï¼ˆ0-10åˆ†ï¼‰ã€‚
    é—®é¢˜: {question}
    å›ç­”: {answer}
    
    åªè¿”å›ä¸€ä¸ª JSON: {{"score": 8.5, "reason": "..."}}
    """
    try:
        resp = await client.post(JUDGE_URL, json={
            "model": JUDGE_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 256
        }, timeout=30)
        res_json = json.loads(resp.json()['choices'][0]['message']['content'].replace("```json", "").replace("```", ""))
        return res_json.get('score', 0)
    except:
        return 0

async def worker(client, sem):
    """æ¨¡æ‹Ÿä¸€ä¸ªä¸æ–­æé—®çš„ç”¨æˆ·"""
    while stats.is_running:
        async with sem:
            question = generate_random_question()
            start = time.perf_counter()
            ttft = 0
            first_chunk = False
            full_response = ""
            
            try:
                # å‘èµ·æµå¼è¯·æ±‚
                async with client.stream("POST", AGENT_URL, json={"message": question, "history": []}, timeout=60) as response:
                    if response.status_code != 200:
                        stats.error_count += 1
                        print(f"{Fore.RED}x", end="", flush=True)
                        continue

                    async for chunk in response.aiter_text():
                        if not first_chunk:
                            ttft = (time.perf_counter() - start) * 1000
                            first_chunk = True
                        if chunk:
                            full_response += chunk
                
                # è¯·æ±‚å®Œæˆ
                total_time = (time.perf_counter() - start) * 1000
                stats.success_count += 1
                stats.latencies.append(total_time)
                stats.ttfts.append(ttft)
                
                # ğŸ² éšæœºæŠ½æ£€è¯„åˆ†
                if random.random() < SAMPLE_RATE:
                    score = await ai_doctor_grade(client, question, full_response)
                    if score > 0:
                        stats.scores.append(score)
                        print(f"{Fore.MAGENTA}â˜…", end="", flush=True) # è¯„åˆ†æ ‡è®°
                    else:
                        print(f"{Fore.GREEN}.", end="", flush=True)
                else:
                    print(f"{Fore.GREEN}.", end="", flush=True) # æˆåŠŸæ ‡è®°

            except Exception as e:
                stats.error_count += 1
                print(f"{Fore.RED}!", end="", flush=True)

async def monitor(duration):
    """ç›‘æ§å€’è®¡æ—¶"""
    start = time.time()
    while time.time() - start < duration:
        await asyncio.sleep(1)
        elapsed = time.time() - start
        rps = stats.success_count / elapsed if elapsed > 0 else 0
        print(f"\r[{elapsed:.0f}s/{duration}s] RPS: {rps:.2f} | Err: {stats.error_count} | Avg Latency: {np.mean(stats.latencies) if stats.latencies else 0:.0f}ms", end="")
    
    stats.is_running = False

async def run_stress_test():
    print(f"{Fore.CYAN}ğŸš€ GIBH-AGENT æŒç»­å‹åŠ›æµ‹è¯• (Sustained Pressure Test)")
    print(f"ç¡¬ä»¶ç¯å¢ƒ: RTX 6000 | å¹¶å‘æ•°: {CONCURRENCY} | æŒç»­æ—¶é—´: {TEST_DURATION_SEC}s")
    print(f"ç­–ç•¥: éšæœºå™ªå£°ç»•è¿‡ç¼“å­˜ + AI Doctor æŠ½æ£€ ({int(SAMPLE_RATE*100)}%)")
    print("-" * 60)
    print("å›¾ä¾‹: .æˆåŠŸ  xå¤±è´¥  !å¼‚å¸¸  â˜…æŠ½æ£€è¯„åˆ†")
    print("-" * 60)

    stats.start_time = time.perf_counter()
    
    # é™åˆ¶è¿æ¥æ± 
    limits = httpx.Limits(max_keepalive_connections=CONCURRENCY, max_connections=CONCURRENCY)
    async with httpx.AsyncClient(limits=limits, timeout=120.0) as client:
        sem = asyncio.Semaphore(CONCURRENCY)
        
        # å¯åŠ¨ç›‘æ§åç¨‹
        monitor_task = asyncio.create_task(monitor(TEST_DURATION_SEC))
        
        # å¯åŠ¨å¹¶å‘ Worker
        workers = [asyncio.create_task(worker(client, sem)) for _ in range(CONCURRENCY)]
        
        # ç­‰å¾…æ—¶é—´ç»“æŸ
        await monitor_task
        
        # ç­‰å¾…æ‰€æœ‰ Worker æ”¶å°¾
        print(f"\n{Fore.YELLOW}â³ æ—¶é—´åˆ°ï¼Œæ­£åœ¨ç­‰å¾…å‰©ä½™è¯·æ±‚å®Œæˆ...")
        await asyncio.gather(*workers, return_exceptions=True)

    print_report()

def print_report():
    total_time = time.perf_counter() - stats.start_time
    
    print("\n\n" + "=" * 60)
    print(f"{Fore.CYAN}ğŸ“Š å‹åŠ›æµ‹è¯•æœ€ç»ˆæŠ¥å‘Š (Pressure Report)")
    print("=" * 60)
    print(f"â±ï¸  å®æµ‹æ—¶é•¿:      {total_time:.2f} s")
    print(f"ğŸ“¦ æ€»è¯·æ±‚æ•°:      {stats.success_count + stats.error_count}")
    print(f"âœ… æˆåŠŸè¯·æ±‚:      {Fore.GREEN}{stats.success_count}")
    print(f"âŒ å¤±è´¥è¯·æ±‚:      {Fore.RED}{stats.error_count}")
    print(f"ğŸš€ å¹³å‡ RPS:      {Fore.YELLOW}{stats.success_count / total_time:.2f} req/s")
    
    if stats.latencies:
        print("-" * 60)
        print(f"âš¡ é¦–å­—å»¶è¿Ÿ (TTFT):")
        print(f"   Avg: {np.mean(stats.ttfts):.2f} ms")
        print(f"   P99: {np.percentile(stats.ttfts, 99):.2f} ms (99%çš„ç”¨æˆ·åœ¨æ­¤æ—¶é—´å†…çœ‹åˆ°ç¬¬ä¸€ä¸ªå­—)")
        
        print(f"ğŸ¢ å®Œæ•´å“åº”è€—æ—¶:")
        print(f"   Avg: {np.mean(stats.latencies):.2f} ms")
        print(f"   P99: {np.percentile(stats.latencies, 99):.2f} ms")
    
    if stats.scores:
        print("-" * 60)
        print(f"ğŸ‘¨â€âš•ï¸ AI Doctor è´¨é‡æŠ½æ£€ ({len(stats.scores)} samples):")
        avg_score = np.mean(stats.scores)
        score_color = Fore.GREEN if avg_score > 8 else Fore.YELLOW
        print(f"   å¹³å‡åˆ†: {score_color}{avg_score:.2f} / 10")
        print(f"   æœ€ä½åˆ†: {min(stats.scores)}")
    
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(run_stress_test())
