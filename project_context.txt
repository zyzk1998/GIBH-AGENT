=== PROJECT STRUCTURE ===
GIBH-AGENT/
    benchmark.py
    architecture_cn.py
    docker-compose.yml
    workflow.py
    manage.sh
    services/
        worker/
            src/
        api/
            Dockerfile
            requirements.txt
            src/
                schemas.py
                celery_app.py
                skill_manager.py
                config.py
                scrna_analysis.py
                main.py
                agent.py
                skills/
                    scanpy_local.py
        nginx/
            html/
                index.html
            conf.d/
                default.conf


--- START OF FILE: benchmark.py ---
import asyncio
import httpx
import time
import json
import random
import string
import pandas as pd
import numpy as np
from colorama import Fore, Style, init
from datetime import datetime

# åˆå§‹åŒ–é¢œè‰²
init(autoreset=True)

# ================= âš™ï¸ å‹æµ‹é…ç½®åŒºåŸŸ (æ ¹æ®éœ€æ±‚è°ƒæ•´) =================
# 1. ç›®æ ‡åœ°å€
AGENT_URL = "http://localhost:8088/api/chat"
JUDGE_URL = "http://localhost:8000/v1/chat/completions"
JUDGE_MODEL = "qwen3-vl"

# 2. å‹åŠ›å‚æ•°
TEST_DURATION_SEC = 120   # â±ï¸ æµ‹è¯•æŒç»­æ—¶é—´ (ç§’)ï¼Œå»ºè®®è®¾ä¸º 300 (5åˆ†é’Ÿ)
CONCURRENCY = 30          # ğŸš€ å¹¶å‘æ•° (RTX 6000 å»ºè®® 30-50ï¼Œå¤ªé«˜ä¼šå¢åŠ å»¶è¿Ÿ)
SAMPLE_RATE = 0.1         # ğŸ” è¯„åˆ†æŠ½æ ·ç‡ (10% çš„å›ç­”ä¼šè¢« AI Doctor æ£€æŸ¥)

# 3. é¢˜åº“ (åŸºç¡€é¢˜ + éšæœºå™ªå£° = æ— é™é¢˜åº“)
BASE_QUESTIONS = [
    "è§£é‡Šå•ç»†èƒæµ‹åºä¸­ Batch Effect çš„åŸç†åŠå»é™¤æ–¹æ³•ã€‚",
    "å¦‚ä½•ä½¿ç”¨ Scanpy è¿›è¡Œç»†èƒèšç±»ï¼Ÿè¯·ç»™å‡ºä»£ç ç¤ºä¾‹ã€‚",
    "TP53 åŸºå› åœ¨è‚¿ç˜¤å‘ç”Ÿä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
    "ä»€ä¹ˆæ˜¯ UMAPï¼Ÿå®ƒå’Œ t-SNE æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
    "çº¿ç²’ä½“åŸºå› å«é‡è¿‡é«˜è¯´æ˜äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
    "è¯·è§£é‡Š Seurat æµç¨‹ä¸­çš„ Normalization æ­¥éª¤ã€‚",
    "ä»€ä¹ˆæ˜¯é«˜å˜åŸºå›  (HVG)ï¼Ÿä¸ºä»€ä¹ˆè¦ç­›é€‰å®ƒä»¬ï¼Ÿ",
    "ç»†èƒå‘¨æœŸè¯„åˆ† (Cell Cycle Scoring) æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ",
    "å¦‚ä½•é€šè¿‡æ ‡è®°åŸºå›  (Marker Genes) æ³¨é‡Šç»†èƒç±»å‹ï¼Ÿ",
    "ç®€è¿° scRNA-seq æ•°æ®åˆ†æçš„æ ‡å‡†æµç¨‹ã€‚"
]
# ===============================================================

class StressStats:
    def __init__(self):
        self.total_requests = 0
        self.success_count = 0
        self.error_count = 0
        self.latencies = []     # æ€»è€—æ—¶
        self.ttfts = []         # é¦–å­—å»¶è¿Ÿ
        self.scores = []        # è´¨é‡è¯„åˆ†
        self.start_time = 0
        self.is_running = True

stats = StressStats()

def generate_random_question():
    """ç”Ÿæˆå¸¦éšæœºå™ªå£°çš„é—®é¢˜ï¼Œé˜²æ­¢ç¼“å­˜ä½œå¼Š"""
    base_q = random.choice(BASE_QUESTIONS)
    # æ·»åŠ éšæœºåç¼€ï¼Œå¼ºåˆ¶ LLM é‡æ–°è®¡ç®— Attention
    salt = ''.join(random.choices(string.ascii_letters + string.digits, k=8))
    return f"{base_q} (Ref: {salt})"

async def ai_doctor_grade(client, question, answer):
    """AI åŒ»ç”ŸæŠ½æ£€è¯„åˆ†"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä½ç”Ÿä¿¡ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹å›ç­”æ‰“åˆ†ï¼ˆ0-10åˆ†ï¼‰ã€‚
    é—®é¢˜: {question}
    å›ç­”: {answer}
    
    åªè¿”å›ä¸€ä¸ª JSON: {{"score": 8.5, "reason": "..."}}
    """
    try:
        resp = await client.post(JUDGE_URL, json={
            "model": JUDGE_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 256
        }, timeout=30)
        res_json = json.loads(resp.json()['choices'][0]['message']['content'].replace("```json", "").replace("```", ""))
        return res_json.get('score', 0)
    except:
        return 0

async def worker(client, sem):
    """æ¨¡æ‹Ÿä¸€ä¸ªä¸æ–­æé—®çš„ç”¨æˆ·"""
    while stats.is_running:
        async with sem:
            question = generate_random_question()
            start = time.perf_counter()
            ttft = 0
            first_chunk = False
            full_response = ""
            
            try:
                # å‘èµ·æµå¼è¯·æ±‚
                async with client.stream("POST", AGENT_URL, json={"message": question, "history": []}, timeout=60) as response:
                    if response.status_code != 200:
                        stats.error_count += 1
                        print(f"{Fore.RED}x", end="", flush=True)
                        continue

                    async for chunk in response.aiter_text():
                        if not first_chunk:
                            ttft = (time.perf_counter() - start) * 1000
                            first_chunk = True
                        if chunk:
                            full_response += chunk
                
                # è¯·æ±‚å®Œæˆ
                total_time = (time.perf_counter() - start) * 1000
                stats.success_count += 1
                stats.latencies.append(total_time)
                stats.ttfts.append(ttft)
                
                # ğŸ² éšæœºæŠ½æ£€è¯„åˆ†
                if random.random() < SAMPLE_RATE:
                    score = await ai_doctor_grade(client, question, full_response)
                    if score > 0:
                        stats.scores.append(score)
                        print(f"{Fore.MAGENTA}â˜…", end="", flush=True) # è¯„åˆ†æ ‡è®°
                    else:
                        print(f"{Fore.GREEN}.", end="", flush=True)
                else:
                    print(f"{Fore.GREEN}.", end="", flush=True) # æˆåŠŸæ ‡è®°

            except Exception as e:
                stats.error_count += 1
                print(f"{Fore.RED}!", end="", flush=True)

async def monitor(duration):
    """ç›‘æ§å€’è®¡æ—¶"""
    start = time.time()
    while time.time() - start < duration:
        await asyncio.sleep(1)
        elapsed = time.time() - start
        rps = stats.success_count / elapsed if elapsed > 0 else 0
        print(f"\r[{elapsed:.0f}s/{duration}s] RPS: {rps:.2f} | Err: {stats.error_count} | Avg Latency: {np.mean(stats.latencies) if stats.latencies else 0:.0f}ms", end="")
    
    stats.is_running = False

async def run_stress_test():
    print(f"{Fore.CYAN}ğŸš€ GIBH-AGENT æŒç»­å‹åŠ›æµ‹è¯• (Sustained Pressure Test)")
    print(f"ç¡¬ä»¶ç¯å¢ƒ: RTX 6000 | å¹¶å‘æ•°: {CONCURRENCY} | æŒç»­æ—¶é—´: {TEST_DURATION_SEC}s")
    print(f"ç­–ç•¥: éšæœºå™ªå£°ç»•è¿‡ç¼“å­˜ + AI Doctor æŠ½æ£€ ({int(SAMPLE_RATE*100)}%)")
    print("-" * 60)
    print("å›¾ä¾‹: .æˆåŠŸ  xå¤±è´¥  !å¼‚å¸¸  â˜…æŠ½æ£€è¯„åˆ†")
    print("-" * 60)

    stats.start_time = time.perf_counter()
    
    # é™åˆ¶è¿æ¥æ± 
    limits = httpx.Limits(max_keepalive_connections=CONCURRENCY, max_connections=CONCURRENCY)
    async with httpx.AsyncClient(limits=limits, timeout=120.0) as client:
        sem = asyncio.Semaphore(CONCURRENCY)
        
        # å¯åŠ¨ç›‘æ§åç¨‹
        monitor_task = asyncio.create_task(monitor(TEST_DURATION_SEC))
        
        # å¯åŠ¨å¹¶å‘ Worker
        workers = [asyncio.create_task(worker(client, sem)) for _ in range(CONCURRENCY)]
        
        # ç­‰å¾…æ—¶é—´ç»“æŸ
        await monitor_task
        
        # ç­‰å¾…æ‰€æœ‰ Worker æ”¶å°¾
        print(f"\n{Fore.YELLOW}â³ æ—¶é—´åˆ°ï¼Œæ­£åœ¨ç­‰å¾…å‰©ä½™è¯·æ±‚å®Œæˆ...")
        await asyncio.gather(*workers, return_exceptions=True)

    print_report()

def print_report():
    total_time = time.perf_counter() - stats.start_time
    
    print("\n\n" + "=" * 60)
    print(f"{Fore.CYAN}ğŸ“Š å‹åŠ›æµ‹è¯•æœ€ç»ˆæŠ¥å‘Š (Pressure Report)")
    print("=" * 60)
    print(f"â±ï¸  å®æµ‹æ—¶é•¿:      {total_time:.2f} s")
    print(f"ğŸ“¦ æ€»è¯·æ±‚æ•°:      {stats.success_count + stats.error_count}")
    print(f"âœ… æˆåŠŸè¯·æ±‚:      {Fore.GREEN}{stats.success_count}")
    print(f"âŒ å¤±è´¥è¯·æ±‚:      {Fore.RED}{stats.error_count}")
    print(f"ğŸš€ å¹³å‡ RPS:      {Fore.YELLOW}{stats.success_count / total_time:.2f} req/s")
    
    if stats.latencies:
        print("-" * 60)
        print(f"âš¡ é¦–å­—å»¶è¿Ÿ (TTFT):")
        print(f"   Avg: {np.mean(stats.ttfts):.2f} ms")
        print(f"   P99: {np.percentile(stats.ttfts, 99):.2f} ms (99%çš„ç”¨æˆ·åœ¨æ­¤æ—¶é—´å†…çœ‹åˆ°ç¬¬ä¸€ä¸ªå­—)")
        
        print(f"ğŸ¢ å®Œæ•´å“åº”è€—æ—¶:")
        print(f"   Avg: {np.mean(stats.latencies):.2f} ms")
        print(f"   P99: {np.percentile(stats.latencies, 99):.2f} ms")
    
    if stats.scores:
        print("-" * 60)
        print(f"ğŸ‘¨â€âš•ï¸ AI Doctor è´¨é‡æŠ½æ£€ ({len(stats.scores)} samples):")
        avg_score = np.mean(stats.scores)
        score_color = Fore.GREEN if avg_score > 8 else Fore.YELLOW
        print(f"   å¹³å‡åˆ†: {score_color}{avg_score:.2f} / 10")
        print(f"   æœ€ä½åˆ†: {min(stats.scores)}")
    
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(run_stress_test())

--- END OF FILE: benchmark.py ---

--- START OF FILE: architecture_cn.py ---
from diagrams import Diagram, Cluster, Edge
from diagrams.programming.framework import FastAPI
from diagrams.onprem.network import Nginx
from diagrams.onprem.inmemory import Redis
from diagrams.onprem.queue import Celery
from diagrams.onprem.compute import Server
from diagrams.onprem.client import User
# âœ… ä¿®å¤ç‚¹ï¼šä½¿ç”¨é€šç”¨å­˜å‚¨å›¾æ ‡
from diagrams.generic.storage import Storage

# ================= å…¨å±€é…ç½® =================
# è®¾ç½®ä¸­æ–‡å­—ä½“ (WenQuanYi Zen Hei)
graph_attr = {
    "fontsize": "16",
    "fontname": "WenQuanYi Zen Hei",
    "bgcolor": "white",
    "splines": "ortho", 
    "nodesep": "0.6",
    "ranksep": "0.8",
    "pad": "0.5"
}

node_attr = {
    "fontname": "WenQuanYi Zen Hei",
    "fontsize": "14"
}

edge_attr = {
    "fontname": "WenQuanYi Zen Hei",
    "fontsize": "12"
}

with Diagram("GIBH-AGENT ç³»ç»Ÿæ¶æ„å›¾ (V1.0)", show=False, direction="TB", 
             graph_attr=graph_attr, node_attr=node_attr, edge_attr=edge_attr):
    
    user = User("ç”¨æˆ·ç»ˆç«¯\n(Web/Browser)")

    # === å®¿ä¸»æœºå®¹å™¨ç¯å¢ƒ ===
    with Cluster("Docker å®¹å™¨åŒ–ç¯å¢ƒ (Ubuntu Host)"):
        
        # 1. æ¥å…¥å±‚
        with Cluster("æ¥å…¥å±‚ (Ingress)"):
            gateway = Nginx("æµé‡ç½‘å…³\n(Nginx :8088)")
        
        # 2. ä¸šåŠ¡åº”ç”¨å±‚
        with Cluster("ä¸šåŠ¡åº”ç”¨å±‚ (Application)"):
            api = FastAPI("ä¸»æ§æœåŠ¡\n(API Server)")
            
            with Cluster("å¼‚æ­¥ä»»åŠ¡è°ƒåº¦"):
                queue = Redis("æ¶ˆæ¯é˜Ÿåˆ—\n(Redis Broker)")
                worker = Celery("ç”Ÿä¿¡åˆ†æå¼•æ“\n(Async Worker)")
        
        # 3. æ™ºèƒ½æ¨ç†å±‚
        with Cluster("AI æ¨ç†å±‚ (Intelligence)"):
            vllm = Server("å¤§æ¨¡å‹æœåŠ¡\n(vLLM Engine)")
            
        # 4. æ•°æ®å­˜å‚¨å±‚
        with Cluster("æ•°æ®æŒä¹…åŒ– (Persistence)"):
            models = Storage("æœ¬åœ°æ¨¡å‹æƒé‡\n(Qwen3-VL)")
            uploads = Storage("ç”¨æˆ·æ•°æ®/ç»“æœ\n(Shared Volume)")

    # === ç¡¬ä»¶åº•åº§ ===
    gpu = Server("ç®—åŠ›åº•åº§")

    # ================= è¿çº¿é€»è¾‘ (å¸¦ä¸­æ–‡è¯´æ˜) =================

    # 1. è®¿é—®é“¾è·¯
    user >> Edge(label="HTTPè¯·æ±‚") >> gateway
    gateway >> Edge(label="åå‘ä»£ç†") >> api
    gateway >> Edge(style="dotted", label="é™æ€èµ„æºæ˜ å°„") >> uploads

    # 2. ä¸šåŠ¡é“¾è·¯
    api >> Edge(label="1. å®æ—¶å¯¹è¯") >> vllm
    api >> Edge(label="2. æäº¤ä»»åŠ¡") >> queue >> Edge(label="æ¶ˆè´¹") >> worker
    
    # 3. åˆ†æé“¾è·¯
    worker >> Edge(label="è§†è§‰ç†è§£") >> vllm
    worker >> Edge(label="è¯»å†™æ•°æ®") >> uploads

    # 4. ç¡¬ä»¶æ”¯æ’‘
    vllm >> Edge(label="åŠ è½½æƒé‡", style="dashed") >> models
    vllm >> Edge(label="CUDA åŠ é€Ÿ", color="firebrick", penwidth="2.0") >> gpu

--- END OF FILE: architecture_cn.py ---

--- START OF FILE: docker-compose.yml ---
version: '3.9'

services:
  # =========================================
  # 1. Nginx ç½‘å…³
  # =========================================
  nginx:
    image: nginx:alpine
    container_name: gibh_gateway
    restart: always
    ports:
      - "8088:80"
    volumes:
      - ./services/nginx/conf.d:/etc/nginx/conf.d
      - ./services/nginx/html:/usr/share/nginx/html
      - ./data/uploads:/app/uploads
    depends_on:
      - api-server

  # =========================================
  # 2. API æœåŠ¡
  # =========================================
  api-server:
    build: 
      context: ./services/api
      dockerfile: Dockerfile
    image: gibh-api:latest
    container_name: gibh_api
    restart: always
    environment:
      - REDIS_URL=redis://redis:6379/0
      - VLLM_URL=http://inference-engine:8000/v1
      - UPLOAD_DIR=/app/uploads
      - LLM_MODEL=qwen3-vl
    volumes:
      - ./services/api/src:/app/src
      - ./data/uploads:/app/uploads
    depends_on:
      - redis
      - inference-engine

  # =========================================
  # 3. Worker æœåŠ¡
  # =========================================
  worker:
    image: gibh-api:latest
    container_name: gibh_worker
    restart: always
    command: celery -A src.celery_app worker --loglevel=info --concurrency=2
    environment:
      - REDIS_URL=redis://redis:6379/0
      - UPLOAD_DIR=/app/uploads
      - VLLM_URL=http://inference-engine:8000/v1
      - LLM_MODEL=qwen3-vl
      - PYTHONDONTWRITEBYTECODE=1
    volumes:
      - ./services/api/src:/app/src
      - ./data/uploads:/app/uploads
    depends_on:
      - redis
      - api-server

  # =========================================
  # 4. Redis
  # =========================================
  redis:
    image: redis:7-alpine
    container_name: gibh_redis
    restart: always
    volumes:
      - ./data/redis:/data

  # =========================================
  # 5. æ¨ç†å¼•æ“ (vLLM - latest)
  # =========================================
  inference-engine:
    # ğŸ”´ ä½¿ç”¨ä½ åˆšåˆšæ‹‰å–æˆåŠŸçš„ç‰ˆæœ¬
    image: vllm/vllm-openai:latest
    container_name: gibh_inference
    runtime: nvidia
    restart: always
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_TRUST_REMOTE_CODE=True
      - VLLM_USE_V1=1
    volumes:
      - ./data/models/Qwen3-VL-8B-Instruct:/model
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8000:8000"
    command: >
      --model /model
      --served-model-name qwen3-vl
      --gpu-memory-utilization 0.9
      --max-model-len 32768
      --enforce-eager
      --trust-remote-code
      --disable-log-stats
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]


--- END OF FILE: docker-compose.yml ---

--- START OF FILE: workflow.py ---
# éœ€è¦å®‰è£…: pip install diagrams
from diagrams import Diagram, Cluster, Edge
from diagrams.onprem.compute import Server
from diagrams.onprem.network import Nginx
from diagrams.onprem.inmemory import Redis
from diagrams.onprem.queue import Celery
from diagrams.programming.framework import FastAPI
from diagrams.custom import Custom

# ç”šè‡³å¯ä»¥ç”¨è‡ªå®šä¹‰å›¾ç‰‡åšå›¾æ ‡ (æ¯”å¦‚ vLLM)
with Diagram("GIBH-AGENT Architecture", show=False, direction="TB"):
    user = Server("User")
    
    with Cluster("Docker Host"):
        ingress = Nginx("Gateway :8088")
        
        with Cluster("Application Layer"):
            api = FastAPI("API Server")
            worker = Celery("Async Worker")
            redis = Redis("Broker")
            
        with Cluster("Inference Layer"):
            # è¿™é‡Œå¯ä»¥ç”¨ Custom åŠ è½½ vLLM çš„ logoï¼Œæˆ–è€…ç”¨é€šç”¨å›¾æ ‡
            vllm = Server("vLLM Engine")
            
    gpu = Server("RTX 6000 Blackwell")

    # é€»è¾‘è¿çº¿
    user >> Edge(label="HTTP") >> ingress
    ingress >> api
    api >> Edge(label="Task") >> redis >> worker
    api >> Edge(label="Chat") >> vllm
    worker >> Edge(label="Analysis") >> vllm
    vllm >> Edge(label="CUDA 12.8") >> gpu

--- END OF FILE: workflow.py ---

--- START OF FILE: manage.sh ---
#!/bin/bash

# ====================================================
# GIBH-AGENT è¿ç»´ç®¡ç†è„šæœ¬ (DevOps Tool)
# ====================================================

# å®šä¹‰é¢œè‰²
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# æ£€æŸ¥æ˜¯å¦åœ¨é¡¹ç›®æ ¹ç›®å½•
if [ ! -f "docker-compose.yml" ]; then
    echo -e "${RED}âŒ é”™è¯¯ï¼šè¯·åœ¨åŒ…å« docker-compose.yml çš„é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬ï¼${NC}"
    exit 1
fi

# æ‰“å°å¤´éƒ¨
print_header() {
    clear
    echo -e "${BLUE}================================================${NC}"
    echo -e "${CYAN}   ğŸ§¬ GIBH-AGENT æ™ºèƒ½ä½“ç®¡ç†æ§åˆ¶å° (RTX 6000)   ${NC}"
    echo -e "${BLUE}================================================${NC}"
}

# æ ¸å¿ƒå‡½æ•°
start_all() {
    echo -e "${GREEN}ğŸš€ æ­£åœ¨æ„å»ºå¹¶å¯åŠ¨æ‰€æœ‰æœåŠ¡...${NC}"
    sudo docker compose up -d --build
    echo -e "${GREEN}âœ… æ‰€æœ‰æœåŠ¡å·²å¯åŠ¨ï¼${NC}"
}

stop_all() {
    echo -e "${YELLOW}ğŸ›‘ æ­£åœ¨åœæ­¢æ‰€æœ‰æœåŠ¡...${NC}"
    sudo docker compose down
    echo -e "${GREEN}âœ… æœåŠ¡å·²åœæ­¢ã€‚${NC}"
}

restart_backend() {
    echo -e "${YELLOW}ğŸ”„ æ­£åœ¨é‡å¯ä¸šåŠ¡å±‚ (API + Worker + Nginx)...${NC}"
    echo -e "${CYAN}â„¹ï¸  æç¤ºï¼švLLM (æ¨ç†å¼•æ“) ä¸ä¼šé‡å¯ï¼Œæ— éœ€é‡æ–°åŠ è½½æ¨¡å‹ã€‚${NC}"
    sudo docker compose restart api-server worker nginx
    echo -e "${GREEN}âœ… ä¸šåŠ¡ä»£ç å·²æ›´æ–°å¹¶é‡å¯ã€‚${NC}"
}

restart_vllm() {
    echo -e "${RED}âš ï¸  è­¦å‘Šï¼šé‡å¯ vLLM éœ€è¦é‡æ–°åŠ è½½ 16GB æ¨¡å‹ï¼Œè€—æ—¶è¾ƒé•¿ã€‚${NC}"
    read -p "ç¡®è®¤é‡å¯? (y/n): " confirm
    if [[ $confirm == "y" ]]; then
        sudo docker compose restart inference-engine
        echo -e "${GREEN}âœ… vLLM å·²é‡å¯ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—ç­‰å¾…æ¨¡å‹åŠ è½½ã€‚${NC}"
        view_logs_vllm
    fi
}

view_logs_vllm() {
    echo -e "${CYAN}ğŸ“œ æ­£åœ¨æ‰“å¼€ vLLM æ¨ç†å¼•æ“æ—¥å¿— (æŒ‰ Ctrl+C é€€å‡º)...${NC}"
    sudo docker compose logs -f inference-engine
}

view_logs_api() {
    echo -e "${CYAN}ğŸ“œ æ­£åœ¨æ‰“å¼€ API & Worker è”åˆæ—¥å¿— (æŒ‰ Ctrl+C é€€å‡º)...${NC}"
    sudo docker compose logs -f api-server worker
}

check_status() {
    echo -e "${BLUE}ğŸ“Š å®¹å™¨è¿è¡ŒçŠ¶æ€ï¼š${NC}"
    sudo docker compose ps
    echo ""
    echo -e "${BLUE}ğŸ® æ˜¾å¡çŠ¶æ€ (nvidia-smi)ï¼š${NC}"
    nvidia-smi
    read -p "æŒ‰å›è½¦é”®è¿”å›èœå•..."
}

# ä¸»å¾ªç¯
while true; do
    print_header
    echo -e "1. ${GREEN}ğŸš€ ä¸€é”®å¯åŠ¨ (Build & Up)${NC}  - åˆæ¬¡è¿è¡Œæˆ–ä¿®æ”¹é…ç½®åç”¨"
    echo -e "2. ${YELLOW}ğŸ›‘ åœæ­¢æ‰€æœ‰æœåŠ¡ (Down)${NC}    - å½»åº•å…³é—­"
    echo -e "3. ${CYAN}ğŸ”„ çƒ­é‡å¯åç«¯ä»£ç ${NC}         - ä¿®æ”¹ Python/HTML ä»£ç åç”¨ (å¿«!)"
    echo -e "4. ${RED}ğŸ”¥ é‡å¯æ¨ç†å¼•æ“ (vLLM)${NC}    - æ˜¾å¡æŠ¥é”™æˆ–æ¨¡å‹å¡æ­»æ—¶ç”¨"
    echo -e "------------------------------------------------"
    echo -e "5. ${BLUE}ğŸ“œ æŸ¥çœ‹ vLLM æ—¥å¿—${NC}           - çœ‹æ¨¡å‹åŠ è½½è¿›åº¦/æ˜¾å­˜æŠ¥é”™"
    echo -e "6. ${BLUE}ğŸ“œ æŸ¥çœ‹ ä¸šåŠ¡ä»£ç  æ—¥å¿—${NC}       - çœ‹ API æŠ¥é”™/ä»»åŠ¡æ‰§è¡Œæƒ…å†µ"
    echo -e "7. ${BLUE}ğŸ“Š æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€${NC}             - Docker çŠ¶æ€ + æ˜¾å¡è´Ÿè½½"
    echo -e "------------------------------------------------"
    echo -e "0. ğŸšª é€€å‡ºè„šæœ¬"
    echo -e "================================================"
    read -p "è¯·è¾“å…¥é€‰é¡¹æ•°å­— [0-7]: " choice

    case $choice in
        1) start_all; read -p "æŒ‰å›è½¦ç»§ç»­..." ;;
        2) stop_all; read -p "æŒ‰å›è½¦ç»§ç»­..." ;;
        3) restart_backend; read -p "æŒ‰å›è½¦ç»§ç»­..." ;;
        4) restart_vllm; read -p "æŒ‰å›è½¦ç»§ç»­..." ;;
        5) view_logs_vllm ;;
        6) view_logs_api ;;
        7) check_status ;;
        0) echo "ğŸ‘‹ Bye!"; exit 0 ;;
        *) echo -e "${RED}æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡è¯•ã€‚${NC}"; sleep 1 ;;
    esac
done

--- END OF FILE: manage.sh ---

--- START OF FILE: services/api/Dockerfile ---
FROM python:3.12-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿçº§ä¾èµ– (ç¼–è¯‘ç”Ÿä¿¡åº“éœ€è¦)
RUN apt-get update && apt-get install -y \
    build-essential gcc libhdf5-dev \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

# é»˜è®¤å¯åŠ¨å‘½ä»¤ (ä¼šè¢« docker-compose è¦†ç›–)
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]

--- END OF FILE: services/api/Dockerfile ---

--- START OF FILE: services/api/requirements.txt ---
# Web æ¡†æ¶
fastapi>=0.115.0
uvicorn[standard]>=0.34.0
gunicorn>=23.0.0
python-multipart>=0.0.9
python-dotenv>=1.0.1
requests>=2.31.0

# AI & LangChain
langchain>=1.0.0
langchain-community>=0.3.0
langchain-openai>=0.2.0
langchain-chroma>=0.1.4
chromadb>=0.6.0

# å¼‚æ­¥ä»»åŠ¡ & æ•°æ®
celery[redis]>=5.5.0
redis>=6.0.0
pydantic>=2.10.0
pydantic-settings>=2.7.0

# ç”Ÿä¿¡åˆ†æ (æ ¸å¿ƒä¿®å¤ç‚¹)
bioblend>=1.4.0
scanpy[leiden]>=1.10.0  # ğŸ‘ˆ å–æ¶ˆæ³¨é‡Šï¼Œå¹¶å¼ºåˆ¶å®‰è£… leiden èšç±»ç®—æ³•
matplotlib>=3.8.0
pandas>=2.1.0
scipy>=1.11.0

--- END OF FILE: services/api/requirements.txt ---

--- START OF FILE: services/api/src/schemas.py ---
from pydantic import BaseModel
from typing import List, Optional, Dict, Any

class FileInfo(BaseModel):
    id: str
    name: str

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, str]] = []
    # å¯¹åº”å‰ç«¯çš„ selectedTool
    selected_tool: Optional[Dict[str, Any]] = None
    # å¯¹åº”å‰ç«¯çš„ toolParams
    tool_params: Optional[Dict[str, Any]] = None
    # å¯¹åº”å‰ç«¯çš„ workflowData
    workflow_data: Optional[Dict[str, Any]] = None
    # å¯¹åº”å‰ç«¯çš„ uploadedFiles
    uploaded_files: List[FileInfo] = []
    # å¯¹åº”å‰ç«¯çš„ useHistoryFiles
    use_history_files: bool = False

--- END OF FILE: services/api/src/schemas.py ---

--- START OF FILE: services/api/src/celery_app.py ---
from celery import Celery
from .config import settings
from .skill_manager import SkillManager 
import os

celery_app = Celery(
    "gibh_worker",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL
)

skill_mgr = SkillManager()

@celery_app.task(bind=True)
def run_bioinformatics_task(self, workflow_data: dict, files: list):
    """
    ç»Ÿä¸€ä»»åŠ¡å…¥å£
    """
    print(f"ğŸš€ [Worker] æ”¶åˆ°ä»»åŠ¡ï¼Œæ–‡ä»¶åˆ—è¡¨: {[f.get('name') for f in files]}")
    
    # ğŸ›‘ å¼ºåˆ¶èµ°æœ¬åœ°åˆ†æé€»è¾‘ (ç§»é™¤ Galaxy åˆ†æ”¯ï¼Œé˜²æ­¢å‡æˆåŠŸ)
    return _run_local_skill(self, workflow_data, files)

def _run_local_skill(task_instance, workflow_data, files):
    """æ‰§è¡Œæœ¬åœ° Python æ’ä»¶"""
    
    if not files:
        return {"status": "failed", "error": "âŒ é”™è¯¯ï¼šæœªæ¥æ”¶åˆ°æ–‡ä»¶ä¿¡æ¯ã€‚"}
    
    # === ğŸ“‚ æ™ºèƒ½è·¯å¾„å¤„ç† (å…³é”®ä¿®å¤) ===
    # é»˜è®¤å–ç¬¬ä¸€ä¸ªæ–‡ä»¶
    target_file_name = files[0]['name']
    
    # æ£€æŸ¥æ˜¯å¦ä¸º 10x Genomics æ ¼å¼ (é€šå¸¸åŒ…å« matrix.mtx)
    is_10x = False
    for f in files:
        if 'matrix.mtx' in f['name']:
            is_10x = True
            break
    
    if is_10x:
        # å¦‚æœæ˜¯ 10x æ•°æ®ï¼ŒScanpy éœ€è¦è¯»å–çš„æ˜¯"ç›®å½•"ï¼Œè€Œä¸æ˜¯æ–‡ä»¶
        # Docker é‡Œçš„ä¸Šä¼ ç›®å½•æ˜¯ settings.UPLOAD_DIR
        data_input_path = settings.UPLOAD_DIR
        print(f"ğŸ“‚ æ£€æµ‹åˆ° 10x æ ¼å¼ï¼Œè®¾ç½®è¾“å…¥è·¯å¾„ä¸ºç›®å½•: {data_input_path}")
    else:
        # å¦‚æœæ˜¯ h5adï¼Œç›´æ¥è¯»å–æ–‡ä»¶
        data_input_path = os.path.join(settings.UPLOAD_DIR, target_file_name)
        print(f"ğŸ“‚ æ£€æµ‹åˆ°å•æ–‡ä»¶æ ¼å¼ï¼Œè®¾ç½®è¾“å…¥è·¯å¾„ä¸º: {data_input_path}")

    # å†æ¬¡æ ¡éªŒç‰©ç†è·¯å¾„
    if not os.path.exists(data_input_path):
        return {"status": "failed", "error": f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è·¯å¾„ {data_input_path}"}
    
    # æå–å‚æ•°
    merged_params = {}
    for step in workflow_data['steps']:
        merged_params.update(step.get('params', {}))
    
    # è·å–æ’ä»¶
    skill = skill_mgr.get_skill("scanpy_local")
    if not skill:
        # å°è¯•é‡æ–°åŠ è½½
        skill_mgr._load_skills()
        skill = skill_mgr.get_skill("scanpy_local")
        if not skill:
            return {"status": "failed", "error": "âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•åŠ è½½ scanpy_local æ’ä»¶ï¼Œè¯·æ£€æŸ¥ skills ç›®å½•ã€‚"}
    
    # === æ‰§è¡Œ ===
    try:
        print("â–¶ï¸ å¼€å§‹æ‰§è¡Œ Scanpy Pipeline...")
        task_instance.update_state(state='PROGRESS', meta={'steps': [{"name": "æ­£åœ¨åˆå§‹åŒ– Scanpy...", "status": "running"}]})
        
        # æ‰§è¡Œåˆ†æ
        result = skill.execute(data_input_path, merged_params, settings.UPLOAD_DIR)
        
        print(f"âœ… æ‰§è¡Œç»“æŸï¼ŒçŠ¶æ€: {result.get('status')}")
        return result
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"status": "failed", "error": f"è¿è¡Œå¼‚å¸¸: {str(e)}"}

--- END OF FILE: services/api/src/celery_app.py ---

--- START OF FILE: services/api/src/skill_manager.py ---
import os
import importlib.util
import glob
import sys

class SkillManager:
    def __init__(self, skills_dir="skills"):
        # ğŸ›¡ï¸ å¥å£®çš„è·¯å¾„æŸ¥æ‰¾é€»è¾‘
        # 1. å°è¯•ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„è·¯å¾„
        base_dir = os.path.dirname(os.path.abspath(__file__))
        target_dir = os.path.join(base_dir, skills_dir)
        
        # 2. å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ç›¸å¯¹äºå·¥ä½œç›®å½• (Docker å®¹å™¨å†…é€šå¸¸æ˜¯ /app/src/skills)
        if not os.path.exists(target_dir):
            target_dir = os.path.join(os.getcwd(), "src", skills_dir)
        
        # 3. å†æ¬¡å…œåº•
        if not os.path.exists(target_dir):
             target_dir = "/app/src/skills"

        self.skills_dir = target_dir
        self.skills = {}
        print(f"ğŸ” SkillManager initialized. Scanning dir: {self.skills_dir}")
        self._load_skills()

    def _load_skills(self):
        """åŠ¨æ€åŠ è½½ skills ç›®å½•ä¸‹çš„æ‰€æœ‰ .py æ’ä»¶"""
        if not os.path.exists(self.skills_dir):
            print(f"âš ï¸ Skills dir not found: {self.skills_dir}")
            return

        # å°† skills ç›®å½•åŠ å…¥ sys.pathï¼Œé˜²æ­¢ import æŠ¥é”™
        if self.skills_dir not in sys.path:
            sys.path.append(self.skills_dir)

        for file_path in glob.glob(os.path.join(self.skills_dir, "*.py")):
            module_name = os.path.basename(file_path)[:-3]
            if module_name == "__init__":
                continue
            
            try:
                spec = importlib.util.spec_from_file_location(module_name, file_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                
                if hasattr(module, "META"):
                    self.skills[module.META['id']] = module
                    print(f"âœ… Loaded Skill: {module.META['name']} ({module.META['id']})")
            except Exception as e:
                print(f"âŒ Failed to load skill {module_name}: {e}")

    def get_skill(self, skill_id):
        return self.skills.get(skill_id)

    def match_skill(self, query):
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        for skill in self.skills.values():
            if skill.META['id'] in query or skill.META['name'] in query:
                return skill
        return None

--- END OF FILE: services/api/src/skill_manager.py ---

--- START OF FILE: services/api/src/config.py ---
import os
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # åŸºç¡€è·¯å¾„
    UPLOAD_DIR: str = "/app/uploads"
    
    # æœåŠ¡è¿æ¥
    REDIS_URL: str = os.getenv("REDIS_URL", "redis://redis:6379/0")
    VLLM_URL: str = os.getenv("VLLM_URL", "http://inference-engine:8000/v1")
    LLM_MODEL: str = os.getenv("LLM_MODEL", "qwen3-vl")
    
    # Galaxy é…ç½® (å¯é€‰)
    GALAXY_URL: str = os.getenv("GALAXY_URL", "http://192.168.8.111:8080")
    GALAXY_KEY: str = os.getenv("GALAXY_API_KEY", "your_key")

settings = Settings()
os.makedirs(settings.UPLOAD_DIR, exist_ok=True)

--- END OF FILE: services/api/src/config.py ---

--- START OF FILE: services/api/src/scrna_analysis.py ---
import scanpy as sc
import os
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
import pandas as pd
import time
import warnings
import io
import base64

warnings.filterwarnings("ignore")

sc.settings.verbosity = 3
sc.settings.set_figure_params(dpi=300, facecolor='white', frameon=True, vector_friendly=True)

class LocalSingleCellPipeline:
    def __init__(self, output_dir="/app/uploads/results"):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)

    def _save_plot(self, name_prefix):
        timestamp = int(time.time())
        filename = f"{name_prefix}_{timestamp}.png"
        save_path = os.path.join(self.output_dir, filename)
        plt.savefig(save_path, bbox_inches='tight', dpi=300)
        plt.close()
        return f"/uploads/results/{filename}"

    def run_pipeline(self, data_input, steps_config=None):
        report = {
            "status": "running",
            "steps_details": [],
            "final_plot": None,
            "qc_metrics": {},
            "diagnosis": "",
            "error": None
        }
        adata = None

        try:
            print(f"ğŸ“‚ Loading data from: {data_input}")
            
            # === ğŸ› ï¸ æ ¸å¿ƒä¿®å¤ï¼šæ›´å¥å£®çš„æ•°æ®è¯»å–é€»è¾‘ ===
            if os.path.isdir(data_input):
                # å°è¯•è¯»å– 10x ç›®å½•
                try:
                    # ä¼˜å…ˆå°è¯•æ ‡å‡†è¯»å– (ä¼šè‡ªåŠ¨æ‰¾ .gz)
                    adata = sc.read_10x_mtx(data_input, var_names='gene_symbols', cache=False)
                except FileNotFoundError:
                    print("âš ï¸ read_10x_mtx failed, trying manual mtx load...")
                    # å¦‚æœå¤±è´¥ (æ¯”å¦‚æ–‡ä»¶æ²¡å‹ç¼©)ï¼Œå°è¯•æ‰‹åŠ¨è¯»å–
                    # å‡è®¾æ–‡ä»¶åæ˜¯æ ‡å‡†çš„ matrix.mtx, features.tsv, barcodes.tsv
                    mtx_path = os.path.join(data_input, "matrix.mtx")
                    if not os.path.exists(mtx_path): mtx_path = os.path.join(data_input, "matrix.mtx.gz")
                    
                    adata = sc.read_mtx(mtx_path).T  # è¯»å…¥åè½¬ç½® (Cells x Genes)
                    
                    # è¯»å– features (genes)
                    genes_path = os.path.join(data_input, "features.tsv")
                    if not os.path.exists(genes_path): genes_path = os.path.join(data_input, "genes.tsv")
                    genes = pd.read_csv(genes_path, header=None, sep='\t')
                    adata.var_names = genes[1].values # å‡è®¾ç¬¬äºŒåˆ—æ˜¯åŸºå› å
                    adata.var['gene_ids'] = genes[0].values
                    
                    # è¯»å– barcodes
                    barcodes_path = os.path.join(data_input, "barcodes.tsv")
                    barcodes = pd.read_csv(barcodes_path, header=None, sep='\t')
                    adata.obs_names = barcodes[0].values
                
                adata.var_names_make_unique()
                
            elif data_input.endswith('.h5ad'):
                adata = sc.read_h5ad(data_input)
            else:
                adata = sc.read(data_input)
            # ===========================================
            
            report["qc_metrics"]["raw_cells"] = adata.n_obs
            report["qc_metrics"]["raw_genes"] = adata.n_vars
            
            if not steps_config: steps_config = []

            for step in steps_config:
                tool_id = step['tool_id']
                params = step.get('params', {})
                step_result = {"name": tool_id, "status": "success", "plot": None, "details": ""}

                print(f"â–¶ï¸ Running step: {tool_id}")

                if tool_id == "local_qc":
                    adata.var['mt'] = adata.var_names.str.startswith(('MT-', 'mt-'))
                    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)
                    sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'], jitter=0.4, multi_panel=True, show=False)
                    step_result["plot"] = self._save_plot("qc_violin")
                    
                    min_genes = int(params.get('min_genes', 200))
                    max_mt = float(params.get('max_mt', 20))
                    sc.pp.filter_cells(adata, min_genes=min_genes)
                    adata = adata[adata.obs.pct_counts_mt < max_mt, :]
                    
                    report["qc_metrics"]["filtered_cells"] = adata.n_obs
                    step_result["summary"] = f"å‰©ä½™ {adata.n_obs} ç»†èƒ"

                elif tool_id == "local_normalize":
                    sc.pp.normalize_total(adata, target_sum=1e4)
                    sc.pp.log1p(adata)
                    step_result["summary"] = "LogNormalize å®Œæˆ"

                elif tool_id == "local_hvg":
                    sc.pp.highly_variable_genes(adata, n_top_genes=2000)
                    sc.pl.highly_variable_genes(adata, show=False)
                    step_result["plot"] = self._save_plot("hvg")
                    adata = adata[:, adata.var.highly_variable]
                    step_result["summary"] = "ç­›é€‰ 2000 é«˜å˜åŸºå› "

                elif tool_id == "local_scale":
                    sc.pp.scale(adata, max_value=10)
                    step_result["summary"] = "æ•°æ®ç¼©æ”¾å®Œæˆ"

                elif tool_id == "local_pca":
                    sc.tl.pca(adata, svd_solver='arpack')
                    sc.pl.pca_variance_ratio(adata, log=True, show=False)
                    step_result["plot"] = self._save_plot("pca_variance")
                    step_result["summary"] = "PCA é™ç»´å®Œæˆ"

                elif tool_id == "local_neighbors":
                    sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)
                    step_result["summary"] = "é‚»æ¥å›¾æ„å»ºå®Œæˆ"

                elif tool_id == "local_cluster":
                    resolution = float(params.get('resolution', 0.5))
                    sc.tl.leiden(adata, resolution=resolution)
                    n_clusters = len(adata.obs['leiden'].unique())
                    step_result["summary"] = f"Leiden èšç±» (Res={resolution}): {n_clusters} ç°‡"

                elif tool_id == "local_umap":
                    sc.tl.umap(adata)
                    fig, ax = plt.subplots(figsize=(8, 6))
                    sc.pl.umap(adata, color=['leiden'], ax=ax, show=False, title="UMAP", legend_loc='on data', frameon=False)
                    umap_path = self._save_plot("final_umap")
                    step_result["plot"] = umap_path
                    report["final_plot"] = umap_path
                    step_result["summary"] = "UMAP ç”Ÿæˆå®Œæ¯•"

                elif tool_id == "local_tsne":
                    if adata.n_obs < 5000: 
                        sc.tl.tsne(adata)
                        fig, ax = plt.subplots(figsize=(8, 6))
                        sc.pl.tsne(adata, color=['leiden'], ax=ax, show=False, title="t-SNE", frameon=False)
                        step_result["plot"] = self._save_plot("final_tsne")
                        step_result["summary"] = "t-SNE ç”Ÿæˆå®Œæ¯•"
                    else:
                        step_result["summary"] = "ç»†èƒæ•°è¿‡å¤šï¼Œè·³è¿‡ t-SNE"

                elif tool_id == "local_markers":
                    sc.tl.rank_genes_groups(adata, 'leiden', method='t-test')
                    result = adata.uns['rank_genes_groups']
                    groups = result['names'].dtype.names
                    markers_df = pd.DataFrame(
                        {group + '_' + key: result[key][group]
                        for group in groups for key in ['names', 'pvals']}
                    ).head(5)
                    step_result["details"] = markers_df.to_html(classes="table table-sm", index=False)
                    step_result["summary"] = "Marker åŸºå› é‰´å®šå®Œæˆ"

                report["steps_details"].append(step_result)

            report["diagnosis"] = f"""
            ### âœ… åˆ†æå®Œæˆ (10 Steps)
            - **åŸå§‹ç»†èƒ**: {report['qc_metrics'].get('raw_cells', 0)}
            - **è¿‡æ»¤å**: {report['qc_metrics'].get('filtered_cells', 0)}
            - **èšç±»ç»“æœ**: æˆåŠŸè¯†åˆ«å‡ºç»†èƒäºšç¾¤ã€‚
            - **å¯è§†åŒ–**: å·²ç”Ÿæˆ UMAP å’Œ t-SNE (å¦‚é€‚ç”¨) å›¾è¡¨ã€‚
            """
            
            report["status"] = "success"
            return report

        except Exception as e:
            print(f"âŒ Pipeline Error: {e}")
            import traceback
            traceback.print_exc()
            report["status"] = "failed"
            report["error"] = str(e)
            return report

--- END OF FILE: services/api/src/scrna_analysis.py ---

--- START OF FILE: services/api/src/main.py ---
import os
import shutil
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from celery.result import AsyncResult

from .config import settings
from .schemas import ChatRequest
from .agent import BioBlendAgent
from .celery_app import celery_app, run_bioinformatics_task

app = FastAPI(title="GIBH Commercial API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

agent = BioBlendAgent()

@app.post("/api/chat")
async def chat_endpoint(req: ChatRequest):
    """
    å¤„ç†ç”¨æˆ·å¯¹è¯æˆ–å·¥ä½œæµæ‰§è¡Œè¯·æ±‚
    """
    # ğŸŸ¢ åˆ†æ”¯ A: ç”¨æˆ·ç‚¹å‡»äº†â€œè¿è¡Œå·¥ä½œæµâ€
    if req.workflow_data:
        task = run_bioinformatics_task.delay(
            workflow_data=req.workflow_data,
            files=[f.dict() for f in req.uploaded_files]
        )
        return {
            "type": "workflow_started",
            "run_id": task.id,
            "reply": f"ğŸš€ å·¥ä½œæµå·²å¯åŠ¨ï¼ä»»åŠ¡ID: {task.id}\næ­£åœ¨åå°è®¡ç®—ï¼Œè¯·ç¨å€™...",
            "thought": "ä»»åŠ¡å·²æäº¤è‡³ Celery åˆ†å¸ƒå¼é˜Ÿåˆ—ã€‚"
        }

    # ğŸ”µ åˆ†æ”¯ B: æ™ºèƒ½å¯¹è¯ / æ„å›¾è¯†åˆ«
    response = await agent.process_query(
        query=req.message,
        history=req.history,
        uploaded_files=req.uploaded_files
    )
    
    # åˆ¤æ–­è¿”å›ç±»å‹
    if hasattr(response, "__aiter__"):
        return StreamingResponse(response, media_type="text/plain")
    
    return response

@app.post("/api/upload")
async def upload_file(file: UploadFile = File(...)):
    try:
        os.makedirs(settings.UPLOAD_DIR, exist_ok=True)
        file_path = os.path.join(settings.UPLOAD_DIR, file.filename)
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        return {"status": "success", "file_name": file.filename, "file_id": file.filename}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/api/workflow/status/{run_id}")
async def get_status(run_id: str):
    task_result = AsyncResult(run_id, app=celery_app)
    
    response = {
        "status": "running",
        "completed": False,
        "steps_status": [], 
        "error": None
    }

    if task_result.state == 'PENDING':
        response["status"] = "running"
        
    elif task_result.state == 'SUCCESS':
        response["status"] = "success"
        response["completed"] = True
        
        result_data = task_result.result 
        if result_data:
            # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šå°† Worker çš„ç»“æœï¼ˆåŒ…å«å›¾ç‰‡è·¯å¾„ï¼‰é€ä¼ ç»™å‰ç«¯
            response["report_data"] = result_data
            
            # å…¼å®¹è¿›åº¦æ¡æ˜¾ç¤º
            if "steps_details" in result_data:
                response["steps_status"] = result_data["steps_details"]
            elif "steps" in result_data:
                response["steps_status"] = result_data["steps"]
                
    elif task_result.state == 'FAILURE':
        response["status"] = "failed"
        response["completed"] = True
        response["error"] = str(task_result.result)
        
    elif task_result.state == 'PROGRESS':
        info = task_result.info
        if isinstance(info, dict):
            response["steps_status"] = info.get("steps", [])
    
    return response


--- END OF FILE: services/api/src/main.py ---

--- START OF FILE: services/api/src/agent.py ---
import json
import asyncio
from typing import AsyncGenerator, Union
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage
from .config import settings

class BioBlendAgent:
    def __init__(self):
        # è¿æ¥åˆ° vLLM (RTX 6000)
        self.llm = ChatOpenAI(
            model=settings.LLM_MODEL,
            base_url=settings.VLLM_URL,
            api_key="EMPTY",
            temperature=0.1, 
            max_tokens=4096,
            streaming=True
        )

    async def process_query(self, query: str, history: list, uploaded_files: list = None) -> Union[dict, AsyncGenerator]:
        """
        æ™ºèƒ½å¤„ç†å…¥å£
        """
        query_text = query.lower().strip()
        
        # 1. æ˜¾å¼æ„å›¾è¯†åˆ«
        if any(k in query_text for k in ["è§„åˆ’", "æµç¨‹", "workflow", "pipeline"]):
            return self._generate_workflow_config(query, uploaded_files)

        # 2. éšå¼æ„å›¾è¯†åˆ« (Context Awareness)
        if uploaded_files and (not query_text or query_text == "å‘é€äº†æ–‡ä»¶" or len(query_text) < 5):
            if history and len(history) > 0:
                last_msg = history[-1]
                if last_msg.get('role') == 'assistant' and "æœªä¸Šä¼ æ•°æ®" in last_msg.get('content', ''):
                    return self._generate_workflow_config("è§„åˆ’æµç¨‹", uploaded_files)

        # 3. é»˜è®¤ï¼šæµå¼å¯¹è¯ (å¸¦æ·±åº¦æ€è€ƒ)
        return self._stream_chat(query, uploaded_files)

    def _get_filename(self, f):
        if isinstance(f, dict):
            return f.get('name', 'unknown')
        return getattr(f, 'name', 'unknown')

    def _generate_workflow_config(self, query, uploaded_files=None):
        """
        ç”Ÿæˆå‰ç«¯å¯æ¸²æŸ“çš„å·¥ä½œæµé…ç½®å¡ç‰‡
        """
        if not uploaded_files:
            reply_text = "å·²ä¸ºæ‚¨è§„åˆ’æ ‡å‡†åˆ†ææµç¨‹ã€‚âš ï¸ **æ£€æµ‹åˆ°æ‚¨å°šæœªä¸Šä¼ æ•°æ®**ï¼Œè¯·å…ˆç‚¹å‡»å›å½¢é’ˆä¸Šä¼  .h5ad æˆ– .mtx æ–‡ä»¶ï¼Œç„¶åå†ç‚¹å‡»â€œæ‰§è¡Œâ€ã€‚"
        else:
            names = [self._get_filename(f) for f in uploaded_files]
            file_names = ", ".join(names)
            reply_text = f"æ”¶åˆ°æ–‡ä»¶ï¼š**{file_names}**ã€‚\nå·²ä¸ºæ‚¨è‡ªåŠ¨åŒ¹é… **Standard Scanpy Pipeline (10 Steps)**ï¼Œæ¶µç›–ä»è´¨æ§åˆ°å¤šç»´å¯è§†åŒ–çš„å…¨æµç¨‹ã€‚è¯·ç¡®è®¤å‚æ•°ï¼š"

        return {
            "type": "workflow_config",
            "reply": reply_text,
            "workflow_name": "Standard Scanpy Pipeline",
            "steps": [
                {"name": "1. Quality Control", "tool_id": "local_qc", "desc": "Filter cells & genes", "params": [{"name": "min_genes", "label": "Min Genes", "value": "200", "type": "text"}, {"name": "max_mt", "label": "Max MT%", "value": "20", "type": "text"}]},
                {"name": "2. Normalization", "tool_id": "local_normalize", "desc": "Log1p Normalize", "params": []},
                {"name": "3. Find Variable Genes", "tool_id": "local_hvg", "desc": "Select top 2000 genes", "params": []},
                {"name": "4. Scale Data", "tool_id": "local_scale", "desc": "Scale to unit variance", "params": []},
                {"name": "5. PCA", "tool_id": "local_pca", "desc": "Dimensionality Reduction", "params": []},
                {"name": "6. Compute Neighbors", "tool_id": "local_neighbors", "desc": "Build neighborhood graph", "params": []},
                {"name": "7. Clustering", "tool_id": "local_cluster", "desc": "Leiden Clustering", "params": [{"name": "resolution", "label": "Resolution", "value": "0.5", "type": "text"}]},
                {"name": "8. UMAP Visualization", "tool_id": "local_umap", "desc": "Non-linear embedding", "params": []},
                {"name": "9. t-SNE Visualization", "tool_id": "local_tsne", "desc": "t-SNE Visualization", "params": []},
                {"name": "10. Find Markers", "tool_id": "local_markers", "desc": "Identify cluster markers", "params": []}
            ],
            "thought": "è¯†åˆ«åˆ°ç”¨æˆ·éœ€è¦è§„åˆ’åˆ†ææµç¨‹ï¼Œå·²åŠ è½½ Scanpy å®Œæ•´æ ‡å‡†æ¨¡æ¿ã€‚"
        }

    async def _stream_chat(self, query: str, uploaded_files=None) -> AsyncGenerator[str, None]:
        """
        æµå¼å¯¹è¯ç”Ÿæˆå™¨ (æ³¨å…¥æ€ç»´é“¾æŒ‡ä»¤)
        """
        # 1. æ„å»ºä¸Šä¸‹æ–‡
        file_context = ""
        if uploaded_files:
            names = [self._get_filename(f) for f in uploaded_files]
            file_context = f"\n[User Context - Uploaded Files]: {', '.join(names)}"

        # 2. å®šä¹‰ç³»ç»Ÿäººè®¾ (System Prompt) - ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå¼ºåˆ¶è¾“å‡º <think> æ ‡ç­¾
        system_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©ä¿¡æ¯å­¦ä¸“å®¶åŠ©æ‰‹ GIBH-Agentã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å›ç­”é—®é¢˜ï¼š
1. **æ·±åº¦æ€è€ƒ (Thinking Process)**ï¼šé¦–å…ˆï¼Œåœ¨ `<think>` å’Œ `</think>` æ ‡ç­¾å†…ï¼Œè¯¦ç»†è§„åˆ’ä½ çš„å›ç­”é€»è¾‘ã€åˆ†æç”¨æˆ·æ„å›¾ã€æ£€æŸ¥æ˜¯å¦æœ‰æ½œåœ¨çš„å‘ï¼ˆå¦‚æ–‡ä»¶æ ¼å¼ã€å‚æ•°è®¾ç½®ï¼‰ã€‚è¿™éƒ¨åˆ†å†…å®¹ä¸è¦ç›´æ¥å±•ç¤ºç»™ç”¨æˆ·çœ‹ã€‚
2. **æ­£å¼å›ç­” (Response)**ï¼šæ€è€ƒç»“æŸåï¼Œåœ¨æ ‡ç­¾å¤–ç»™å‡ºæœ€ç»ˆçš„ã€ç»“æ„æ¸…æ™°çš„å›ç­”ã€‚

è¯·éµå¾ªï¼š
- å‡†ç¡®æ€§ä¼˜å…ˆï¼Œä¸è¦ç¼–é€ ã€‚
- ä½¿ç”¨ Markdown æ ¼å¼ã€‚
- æ‹’ç»æ— å…³é—®é¢˜ã€‚

{file_context}
"""
        system_message = SystemMessagePromptTemplate.from_template(system_template)
        human_message = HumanMessagePromptTemplate.from_template("{query}")
        
        chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])
        
        chain = chat_prompt | self.llm
        
        # 3. æ‰§è¡Œæµå¼ç”Ÿæˆ
        async for chunk in chain.astream({"query": query, "file_context": file_context}):
            content = ""
            if hasattr(chunk, 'content') and chunk.content:
                content = chunk.content
            elif isinstance(chunk, str):
                content = chunk
            
            if content:
                yield content
                # å¹³æ»‘é˜»å°¼ (RTX 6000 ä¸“ç”¨)
                await asyncio.sleep(0.01)

--- END OF FILE: services/api/src/agent.py ---

--- START OF FILE: services/api/src/skills/scanpy_local.py ---
import sys
import os

# ä¿®å¤å¯¼å…¥è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.dirname(current_dir)
if src_dir not in sys.path:
    sys.path.append(src_dir)

try:
    from scrna_analysis import LocalSingleCellPipeline
except ImportError:
    # Docker ç¯å¢ƒä¸‹çš„å¤‡ç”¨å¯¼å…¥
    from src.scrna_analysis import LocalSingleCellPipeline

META = {
    "id": "scanpy_local",
    "name": "Local Scanpy Engine (Full)",
    "description": "Execute standard 10-step scRNA-seq pipeline",
    "template": {
        "name": "Standard Scanpy Pipeline",
        "steps": [
            {"name": "Quality Control", "tool_id": "local_qc", "params": {"min_genes": "200", "max_mt": "20"}},
            {"name": "Normalization", "tool_id": "local_normalize", "params": {}},
            {"name": "Find Variable Genes", "tool_id": "local_hvg", "params": {}},
            {"name": "Scale Data", "tool_id": "local_scale", "params": {}},
            {"name": "PCA", "tool_id": "local_pca", "params": {}},
            {"name": "Compute Neighbors", "tool_id": "local_neighbors", "params": {}},
            {"name": "Clustering", "tool_id": "local_cluster", "params": {"resolution": "0.5"}},
            {"name": "UMAP Visualization", "tool_id": "local_umap", "params": {}},
            {"name": "t-SNE Visualization", "tool_id": "local_tsne", "params": {}},
            {"name": "Find Markers", "tool_id": "local_markers", "params": {}}
        ]
    }
}

def execute(file_path, params, output_dir):
    print(f"ğŸš€ [Scanpy Skill] Starting analysis on: {file_path}")
    
    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    results_dir = os.path.join(output_dir, "results")
    os.makedirs(results_dir, exist_ok=True)
    
    pipeline = LocalSingleCellPipeline(output_dir=results_dir)
    
    # ä½¿ç”¨ META ä¸­çš„æ¨¡æ¿ä½œä¸ºåŸºå‡†
    steps_config = META['template']['steps']
    
    # æ³¨å…¥ç”¨æˆ·å‚æ•°
    for step in steps_config:
        step_params = step.get('params', {})
        for key in step_params.keys():
            if key in params:
                step['params'][key] = params[key]

    return pipeline.run_pipeline(file_path, steps_config)

--- END OF FILE: services/api/src/skills/scanpy_local.py ---

--- START OF FILE: services/nginx/html/index.html ---
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GIBH Qwen Agent</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.0/font/bootstrap-icons.css">
    <!-- å¼•å…¥ Highlight.js ç”¨äºä»£ç é«˜äº® -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/github-dark.min.css">
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    
    <style>
        /* ================= å…¨å±€å˜é‡ ================= */
        :root {
            --bg-color: #ffffff;
            --chat-width: 800px;
            --primary-color: #4d6bfe;
            --text-color: #1f1f1f;
            --thought-bg: #fcfcfc;
            --thought-border: #f0f0f0;
        }

        body {
            background-color: var(--bg-color);
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            height: 100vh;
            display: flex;
            flex-direction: column;
            margin: 0;
            overflow: hidden;
            color: var(--text-color);
        }

        /* ================= å¯¼èˆªæ  ================= */
        .navbar {
            background: rgba(255,255,255,0.9);
            backdrop-filter: blur(10px);
            position: absolute; top: 0; width: 100%; z-index: 10;
            padding: 12px 0;
            border-bottom: 1px solid rgba(0,0,0,0.03);
        }
        .brand-logo { font-weight: 600; font-size: 1.1rem; color: #333; display: flex; align-items: center; gap: 8px; }

        /* ================= èŠå¤©åŒºåŸŸ ================= */
        .chat-scroll-area {
            flex: 1;
            overflow-y: auto;
            padding: 80px 20px 200px 20px;
            scroll-behavior: smooth;
        }
        .chat-content { max-width: var(--chat-width); margin: 0 auto; }

        /* æ¶ˆæ¯æ°”æ³¡ */
        .message-row { display: flex; margin-bottom: 24px; animation: fadeIn 0.2s ease; }
        .message-row.user { justify-content: flex-end; }
        
        .avatar {
            width: 32px; height: 32px; border-radius: 6px; margin-right: 12px;
            display: flex; align-items: center; justify-content: center;
            font-size: 18px; flex-shrink: 0; margin-top: 2px;
        }
        .avatar.ai { background: #eef2ff; color: var(--primary-color); }
        
        .message-container { max-width: 100%; flex: 1; min-width: 0; }
        .message-row.user .message-container { flex: initial; max-width: 85%; display: flex; flex-direction: column; align-items: flex-end; }

        .bubble { font-size: 15px; line-height: 1.6; color: #374151; position: relative; }
        .message-row.user .bubble {
            background-color: #f4f4f4;
            color: #000;
            padding: 8px 14px;
            border-radius: 18px;
            border-bottom-right-radius: 4px;
            text-align: left;
            display: inline-block;
        }
        .bubble p:last-child { margin-bottom: 0; }
        
        /* ğŸ“‹ ä»£ç å—å¢å¼ºæ ·å¼ */
        .bubble pre {
            background: #0d1117;
            border-radius: 6px;
            margin: 10px 0;
            position: relative;
            padding-top: 30px; /* ç•™å‡ºå¤´éƒ¨ç©ºé—´ */
        }
        .bubble pre code {
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            font-size: 13px;
            padding: 12px;
            display: block;
            overflow-x: auto;
            color: #c9d1d9;
        }
        /* ä»£ç å—å¤´éƒ¨ */
        .code-header {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 30px;
            background: #161b22;
            border-top-left-radius: 6px;
            border-top-right-radius: 6px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0 10px;
            font-size: 12px;
            color: #8b949e;
            border-bottom: 1px solid #30363d;
        }
        .copy-btn {
            background: transparent;
            border: none;
            color: #8b949e;
            cursor: pointer;
            font-size: 12px;
            display: flex;
            align-items: center;
            gap: 4px;
            transition: color 0.2s;
        }
        .copy-btn:hover { color: #fff; }

        /* ğŸ”¥ æ·±åº¦æ€è€ƒæŠ˜å å¡ç‰‡æ ·å¼ */
        .thought-process {
            margin-bottom: 12px;
            border: 1px solid #e5e7eb;
            border-radius: 8px;
            background-color: #f9fafb;
            overflow: hidden;
        }
        .thought-summary {
            padding: 8px 12px;
            cursor: pointer;
            font-size: 12px;
            color: #6b7280;
            display: flex;
            align-items: center;
            background: #f3f4f6;
            user-select: none;
            transition: background 0.2s;
        }
        .thought-summary:hover { background: #e5e7eb; }
        .thought-summary i { margin-right: 6px; color: #8b5cf6; } 
        .thought-content {
            padding: 12px;
            font-size: 13px;
            color: #4b5563;
            line-height: 1.6;
            border-top: 1px solid #e5e7eb;
            display: none; 
            white-space: pre-wrap;
            font-family: 'Menlo', 'Monaco', 'Courier New', monospace;
            background: #ffffff;
        }
        .thought-content.open { display: block; animation: fadeIn 0.3s; }
        
        /* â±ï¸ è®¡æ—¶å™¨ä¸åº•éƒ¨æ  */
        .message-footer {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin-top: 6px;
            font-size: 11px;
            color: #9ca3af;
            padding-left: 2px;
            padding-right: 2px;
        }
        .generation-timer {
            font-family: monospace;
            display: flex;
            align-items: center;
            gap: 4px;
        }
        .generation-timer i { font-size: 10px; }

        /* äº¤äº’ç»„ä»¶ */
        .user-actions { display: flex; flex-direction: row; gap: 15px; opacity: 0; transition: opacity 0.2s; }
        .message-row.user:hover .user-actions { opacity: 1; }
        .action-item { display: flex; align-items: center; gap: 5px; font-size: 12px; color: #9ca3af; cursor: pointer; transition: color 0.2s; white-space: nowrap; }
        .action-item:hover { color: #4b5563; }

        /* åŠŸèƒ½å¡ç‰‡æ ·å¼ */
        .params-form { background: #fff; border: 1px solid #e0e0e0; border-radius: 12px; padding: 20px; margin-top: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.05); }
        .form-label { font-size: 13px; font-weight: 600; color: #555; margin-bottom: 4px; }
        .form-text { font-size: 12px; color: #999; }
        .form-control, .form-select { font-size: 14px; border-radius: 6px; }
        .btn-run { width: 100%; margin-top: 15px; font-weight: 600; }
        
        .accordion-button { padding: 10px 15px; font-size: 14px; font-weight: 600; }
        .accordion-button:not(.collapsed) { background-color: #eef2ff; color: #4d6bfe; }
        .accordion-body { padding: 15px; background: #fcfcfc; }

        .workflow-monitor { background: #fff; border: 1px solid #e0e0e0; border-radius: 12px; padding: 20px; margin-top: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.05); }
        .step-item { display: flex; align-items: center; padding: 10px 0; border-bottom: 1px solid #f0f0f0; }
        .step-item:last-child { border-bottom: none; }
        .step-icon { width: 24px; height: 24px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-right: 12px; font-size: 12px; flex-shrink: 0; }
        .step-icon.pending { background: #f3f4f6; color: #9ca3af; }
        .step-icon.running { background: #eef2ff; color: #4d6bfe; animation: pulse 1.5s infinite; }
        .step-icon.success { background: #d1fae5; color: #059669; }
        .step-icon.failed { background: #fee2e2; color: #dc2626; }
        .step-name { font-size: 14px; font-weight: 500; color: #374151; flex: 1; }
        .step-status-text { font-size: 12px; color: #6b7280; }

        .data-selector-container { background: #fff; border: 1px solid #e0e0e0; border-radius: 12px; overflow: hidden; margin-top: 10px; }
        .data-selector-header { padding: 12px 15px; background: #f9fafb; border-bottom: 1px solid #eee; font-weight: 600; color: #333; font-size: 14px; }
        .data-selector-list { max-height: 300px; overflow-y: auto; padding: 5px 0; }
        .dataset-item { display: flex; align-items: center; padding: 10px 15px; cursor: pointer; transition: background 0.2s; border-bottom: 1px solid #f9f9f9; }
        .dataset-item:last-child { border-bottom: none; }
        .dataset-item:hover { background: #f0f7ff; }
        .dataset-item.selected { background: #eef2ff; border-left: 3px solid var(--primary-color); }
        .dataset-item input { margin-right: 12px; transform: scale(1.2); cursor: pointer; }
        .dataset-info { display: flex; flex-direction: column; pointer-events: none; } 
        .dataset-name { font-size: 13px; font-weight: 500; color: #333; }
        .dataset-meta { font-size: 11px; color: #999; margin-top: 2px; }
        .data-selector-footer { padding: 10px 15px; border-top: 1px solid #eee; background: #fff; display: flex; justify-content: flex-end; }

        .analysis-report { background: #fff; border: 1px solid #e0e0e0; border-radius: 12px; padding: 20px; margin-top: 10px; box-shadow: 0 4px 12px rgba(0,0,0,0.05); }
        .report-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 10px; }
        .report-title { font-size: 16px; font-weight: 700; color: #333; }
        .report-section { margin-top: 20px; }
        .report-section h6 { font-weight: 600; color: #555; margin-bottom: 10px; }
        .llm-diagnosis { background: #f0f7ff; border-left: 4px solid #4d6bfe; padding: 15px; border-radius: 4px; font-size: 14px; line-height: 1.6; color: #333; }

        /* ğŸ” å›¾ç‰‡ç¯ç®±æ ·å¼ */
        .lightbox {
            display: none; position: fixed; z-index: 9999; left: 0; top: 0; width: 100%; height: 100%;
            background-color: rgba(0,0,0,0.9); justify-content: center; align-items: center;
        }
        .lightbox img { max-width: 90%; max-height: 90%; border-radius: 4px; box-shadow: 0 0 20px rgba(255,255,255,0.1); }
        .lightbox-close { position: absolute; top: 20px; right: 30px; color: #f1f1f1; font-size: 40px; font-weight: bold; cursor: pointer; }

        /* æ‰“å°æ ·å¼ */
        @media print {
            @page { margin: 0; size: auto; }
            body * { visibility: hidden; }
            body { margin: 1.6cm; background: white; }
            .analysis-report, .analysis-report * { visibility: visible; }
            .analysis-report { position: absolute; left: 0; top: 0; width: 100%; margin: 0; padding: 0; border: none; box-shadow: none; padding-top: 60px; }
            .report-header button { display: none !important; }
            body::before { content: "GIBH Qwen ç”Ÿä¿¡æ™ºèƒ½åŠ©æ‰‹"; visibility: visible; position: fixed; top: 0; left: 0; right: 0; text-align: center; font-size: 16pt; font-weight: bold; color: #2c3e50; border-bottom: 2px solid #4d6bfe; padding-bottom: 10px; background-color: white; z-index: 9999; }
            body::after { content: "Generated by GIBH Qwen Agent | ä»…ä¾›ç§‘ç ”å‚è€ƒ"; visibility: visible; position: fixed; bottom: 0; left: 0; right: 0; text-align: center; font-size: 9pt; color: #999; background-color: white; padding-top: 5px; border-top: 1px solid #eee; }
            .accordion-collapse { display: block !important; height: auto !important; visibility: visible !important; }
            .accordion-button { pointer-events: none; background: none !important; color: black !important; padding-left: 0; font-weight: bold; border: none; }
            .accordion-button::after { display: none !important; }
            .accordion-item { border: none; border-bottom: 1px solid #eee; }
            img { max-width: 90% !important; display: block; margin: 10px auto; page-break-inside: avoid; }
            * { -webkit-print-color-adjust: exact !important; print-color-adjust: exact !important; }
        }

        /* ================= è¾“å…¥æ¡†åŒºåŸŸ ================= */
        .input-wrapper { position: fixed; bottom: 30px; left: 0; width: 100%; display: flex; justify-content: center; padding: 0 20px; z-index: 100; pointer-events: none; }
        .chat-input-panel { pointer-events: auto; width: 100%; max-width: var(--chat-width); background: #fff; border: 1px solid #e6e6e6; border-radius: 20px; box-shadow: 0 4px 20px rgba(0, 0, 0, 0.05); padding: 10px 14px; display: flex; flex-direction: column; transition: box-shadow 0.2s; }
        .chat-input-panel:focus-within { box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1); border-color: #d0d0d0; }
        .input-area { width: 100%; margin-bottom: 4px; display: flex; align-items: center; }
        .main-input { width: 100%; border: none; outline: none; font-size: 16px; line-height: 1.5; resize: none; color: #333; background: transparent; font-family: inherit; height: 24px; min-height: 24px; max-height: 200px; overflow-y: hidden; padding: 0; margin: 0; box-sizing: border-box; }
        .main-input::placeholder { color: #aaa; }
        .main-input:disabled { background: transparent; cursor: not-allowed; color: #999; }
        .toolbar { display: flex; justify-content: space-between; align-items: center; margin-top: 4px; }
        .toolbar-left { display: flex; gap: 10px; }
        .toolbar-right { display: flex; align-items: center; gap: 10px; }
        .icon-btn { width: 34px; height: 34px; border-radius: 8px; border: none; background: transparent; color: #666; display: flex; align-items: center; justify-content: center; cursor: pointer; transition: background 0.2s, color 0.2s; font-size: 18px; }
        .icon-btn:hover { background: #f5f5f5; color: #333; }
        .icon-btn:disabled { cursor: not-allowed; opacity: 0.5; }
        .send-btn { width: 34px; height: 34px; border-radius: 50%; background: #e5e5e5; color: #fff; border: none; display: flex; align-items: center; justify-content: center; cursor: not-allowed; transition: all 0.2s; margin-left: 8px; }
        .send-btn.active { background: #333; cursor: pointer; }
        .send-btn i { font-size: 16px; }
        
        /* â¹ï¸ åœæ­¢æŒ‰é’®æ ·å¼ */
        .stop-btn {
            display: none; /* é»˜è®¤éšè— */
            background-color: #fee2e2; color: #dc2626; border: 1px solid #fecaca;
            border-radius: 20px; padding: 4px 12px; font-size: 12px;
            cursor: pointer; align-items: center; gap: 4px;
            transition: all 0.2s;
        }
        .stop-btn:hover { background-color: #fecaca; }
        .stop-btn.show { display: flex; }

        .file-chips-area { display: flex; flex-wrap: wrap; gap: 8px; margin-bottom: 4px; }
        .file-chip { background: #f7f7f7; border: 1px solid #eee; border-radius: 6px; padding: 2px 8px; font-size: 12px; color: #555; display: flex; align-items: center; gap: 6px; }
        #dropOverlay { position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: rgba(255,255,255,0.95); z-index: 999; display: none; flex-direction: column; justify-content: center; align-items: center; color: var(--primary-color); }
        @keyframes fadeIn { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }
        @keyframes pulse { 0% { opacity: 1; } 50% { opacity: 0.5; } 100% { opacity: 1; } }
    </style>
</head>
<body>

    <div id="dropOverlay">
        <i class="bi bi-cloud-upload" style="font-size: 48px; margin-bottom: 10px;"></i>
        <h4>é‡Šæ”¾æ–‡ä»¶ä»¥ä¸Šä¼ </h4>
    </div>

    <!-- ğŸ” å›¾ç‰‡ç¯ç®± -->
    <div id="lightbox" class="lightbox" onclick="closeLightbox()">
        <span class="lightbox-close">&times;</span>
        <img id="lightbox-img" src="">
    </div>

    <nav class="navbar">
        <div class="container d-flex justify-content-center">
            <div class="brand-logo"><i class="bi bi-stars" style="color: var(--primary-color);"></i> GIBH Qwen Agent</div>
        </div>
    </nav>

    <div class="chat-scroll-area" id="chatBox">
        <div class="chat-content" id="chatContent">
            <!-- å¼€åœºç™½ -->
            <div class="message-row ai">
                <div class="avatar ai"><i class="bi bi-robot"></i></div>
                <div class="message-container">
                    <div class="bubble">
                        <p>ä½ å¥½ï¼æˆ‘æ˜¯æ‚¨çš„ <b>GIBH Qwen ç”Ÿä¿¡æ™ºèƒ½åŠ©æ‰‹</b>ã€‚</p>
                        <p>æˆ‘å¯ä»¥ååŠ©æ‚¨è¿›è¡Œå·¥å…·æŸ¥è¯¢ã€æ•°æ®ä¸Šä¼ åŠ Seurat æµç¨‹è§„åˆ’ã€‚åŒæ—¶ï¼Œæˆ‘ä¹Ÿå…·å¤‡<b>è§†è§‰åˆ†æèƒ½åŠ›</b>ï¼Œæ‚¨å¯ä»¥å‘é€ï¼š</p>
                        <ul>
                            <li>ğŸ“¸ <b>æŠ¥é”™æˆªå›¾</b>ï¼šè·å–æ•…éšœè¯Šæ–­ä¸ä¿®å¤å»ºè®®ã€‚</li>
                            <li>ğŸ“Š <b>ç”Ÿä¿¡å›¾è¡¨</b>ï¼šè§£è¯» UMAPã€çƒ­å›¾ç­‰æ•°æ®çš„ç”Ÿç‰©å­¦æ„ä¹‰ã€‚</li>
                        </ul>
                        <p style="margin-top: 10px; font-size: 12px; color: #888;">
                            <i class="bi bi-info-circle"></i> è¯•ç€ç›´æ¥æ‹–å…¥ä¸€å¼ å›¾ç‰‡ï¼Œæˆ–è€…é—®æˆ‘ï¼š"å¸®æˆ‘è§„åˆ’ä¸€ä¸ªå•ç»†èƒæµç¨‹"ã€‚
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="input-wrapper">
        <!-- â¹ï¸ åœæ­¢ç”ŸæˆæŒ‰é’® (æ‚¬æµ®åœ¨è¾“å…¥æ¡†ä¸Šæ–¹) -->
        <div id="stopBtnContainer" style="position: absolute; top: -40px; width: 100%; display: flex; justify-content: center; pointer-events: none;">
            <button id="stopBtn" class="stop-btn" style="pointer-events: auto;" onclick="stopGeneration()">
                <i class="bi bi-stop-circle-fill"></i> åœæ­¢ç”Ÿæˆ
            </button>
        </div>

        <div class="chat-input-panel">
            <div id="filePreviewArea" class="file-chips-area"></div>
            <div class="input-area">
                <textarea id="userInput" class="main-input" placeholder="å°½ç®¡é—®..." rows="1" oninput="autoResize(this); checkInput();" onkeydown="handleEnter(event)"></textarea>
            </div>
            <div class="toolbar">
                <div class="toolbar-left"></div>
                <div class="toolbar-right">
                    <input type="file" id="fileInput" style="display: none;" onchange="handleFileSelect(this.files)" multiple>
                    <button id="uploadBtn" class="icon-btn" onclick="document.getElementById('fileInput').click()" title="ä¸Šä¼ æ–‡ä»¶">
                        <i class="bi bi-paperclip"></i>
                    </button>
                    <button id="sendBtn" class="send-btn" onclick="sendMessage()">
                        <i class="bi bi-arrow-up"></i>
                    </button>
                </div>
            </div>
        </div>
    </div>

    <script>
        // é…ç½® marked ä½¿ç”¨ highlight.js
        marked.setOptions({
            highlight: function(code, lang) {
                const language = highlight.getLanguage(lang) ? lang : 'plaintext';
                return highlight.highlight(code, { language }).value;
            },
            langPrefix: 'hljs language-'
        });

        // è‡ªå®šä¹‰æ¸²æŸ“å™¨ï¼šä¸ºä»£ç å—æ·»åŠ å¤´éƒ¨ï¼ˆè¯­è¨€æ ‡ç­¾ + å¤åˆ¶æŒ‰é’®ï¼‰
        const renderer = new marked.Renderer();
        renderer.code = function(code, language) {
            const lang = language || 'Text';
            // è½¬ä¹‰ä»£ç å†…å®¹ä»¥é˜²æ­¢ XSS
            const escapedCode = code.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;").replace(/"/g, "&quot;").replace(/'/g, "&#039;");
            
            return `
            <div class="code-block-wrapper">
                <div class="code-header">
                    <span>${lang}</span>
                    <button class="copy-btn" onclick="copyCode(this)">
                        <i class="bi bi-clipboard"></i> å¤åˆ¶
                    </button>
                </div>
                <pre><code class="hljs language-${lang}">${escapedCode}</code></pre>
                <!-- éšè—çš„åŸå§‹ä»£ç ç”¨äºå¤åˆ¶ -->
                <textarea style="display:none;">${escapedCode}</textarea>
            </div>`;
        };
        marked.use({ renderer });

        const chatContent = document.getElementById('chatContent');
        const chatScrollArea = document.getElementById('chatBox');
        const userInput = document.getElementById('userInput');
        const sendBtn = document.getElementById('sendBtn');
        const uploadBtn = document.getElementById('uploadBtn');
        const filePreviewArea = document.getElementById('filePreviewArea');
        const dropOverlay = document.getElementById('dropOverlay');
        const stopBtn = document.getElementById('stopBtn');

        let chatHistory = []; 
        let uploadedFiles = []; 
        let isProcessing = false; 
        let abortController = null; // ç”¨äºä¸­æ–­è¯·æ±‚

        function autoResize(textarea) {
            textarea.style.height = 'auto';
            textarea.style.height = textarea.scrollHeight + 'px';
            if (textarea.scrollHeight > 200) textarea.style.overflowY = "auto";
            else textarea.style.overflowY = "hidden";
        }
        window.onload = () => autoResize(userInput);

        function checkInput() {
            if (isProcessing) { sendBtn.classList.remove('active'); return; }
            if (userInput.value.trim() !== '' || uploadedFiles.length > 0) sendBtn.classList.add('active');
            else sendBtn.classList.remove('active');
        }

        function handleEnter(e) { 
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                if (!isProcessing) sendMessage();
            }
        }

        function toggleLoadingState(isLoading) {
            isProcessing = isLoading;
            userInput.disabled = isLoading;
            uploadBtn.disabled = isLoading;
            if (isLoading) {
                sendBtn.disabled = true; sendBtn.classList.remove('active'); userInput.style.cursor = 'wait';
                stopBtn.classList.add('show'); // æ˜¾ç¤ºåœæ­¢æŒ‰é’®
            } else {
                sendBtn.disabled = false; userInput.style.cursor = 'text'; userInput.focus(); checkInput();
                stopBtn.classList.remove('show'); // éšè—åœæ­¢æŒ‰é’®
            }
        }

        // â¹ï¸ åœæ­¢ç”Ÿæˆé€»è¾‘
        function stopGeneration() {
            if (abortController) {
                abortController.abort();
                abortController = null;
                toggleLoadingState(false);
                // åœ¨æœ€åä¸€æ¡æ¶ˆæ¯è¿½åŠ  [å·²åœæ­¢]
                const lastBubble = chatContent.lastElementChild.querySelector('.bubble');
                if (lastBubble) lastBubble.innerHTML += ' <span style="color:#dc2626; font-size:12px;">(å·²åœæ­¢)</span>';
            }
        }

        // ğŸ“‹ å¤åˆ¶ä»£ç é€»è¾‘
        function copyCode(btn) {
            const wrapper = btn.closest('.code-block-wrapper');
            const code = wrapper.querySelector('code').innerText;
            navigator.clipboard.writeText(code).then(() => {
                const originalHtml = btn.innerHTML;
                btn.innerHTML = '<i class="bi bi-check2"></i> å·²å¤åˆ¶';
                setTimeout(() => btn.innerHTML = originalHtml, 2000);
            });
        }

        // ğŸ” ç¯ç®±é€»è¾‘
        function openLightbox(src) {
            const lightbox = document.getElementById('lightbox');
            const img = document.getElementById('lightbox-img');
            img.src = src;
            lightbox.style.display = 'flex';
        }
        function closeLightbox() {
            document.getElementById('lightbox').style.display = 'none';
        }

        // æ‹–æ‹½ä¸Šä¼ é€»è¾‘
        window.addEventListener('dragenter', (e) => { if(!isProcessing) { e.preventDefault(); dropOverlay.style.display = 'flex'; } });
        window.addEventListener('dragleave', (e) => { e.preventDefault(); if (e.clientX === 0 && e.clientY === 0) dropOverlay.style.display = 'none'; });
        window.addEventListener('dragover', (e) => e.preventDefault());
        window.addEventListener('drop', (e) => { e.preventDefault(); dropOverlay.style.display = 'none'; if (!isProcessing && e.dataTransfer.files.length > 0) handleFileSelect(e.dataTransfer.files); });
        window.addEventListener('paste', (e) => { if (!isProcessing && e.clipboardData.files.length > 0) { e.preventDefault(); handleFileSelect(e.clipboardData.files); } });

        async function handleFileSelect(files) {
            if (files.length === 0) return;
            toggleLoadingState(true);
            for (let i = 0; i < files.length; i++) await uploadSingleFile(files[i]);
            document.getElementById('fileInput').value = ''; 
            toggleLoadingState(false);
        }

        async function uploadSingleFile(file) {
            const tempId = 'temp-' + Date.now();
            addFileTag(file.name, true, tempId);
            const formData = new FormData();
            formData.append('file', file);
            try {
                const res = await fetch('/api/upload', { method: 'POST', body: formData });
                const data = await res.json();
                removeFileTag(tempId);
                if (data.status === 'success') {
                    uploadedFiles.push({ id: data.file_id, name: data.file_name });
                    renderFileTags();
                } else alert(`ä¸Šä¼ å¤±è´¥: ${data.message}`);
            } catch (e) { removeFileTag(tempId); alert(`ç½‘ç»œé”™è¯¯: ${e}`); }
        }

        function renderFileTags() {
            filePreviewArea.innerHTML = '';
            uploadedFiles.forEach((f, index) => {
                const div = document.createElement('div');
                div.className = 'file-chip';
                div.innerHTML = `<i class="bi bi-file-earmark-text"></i><span>${f.name}</span><i class="bi bi-x" style="cursor:pointer" onclick="removeFile(${index})"></i>`;
                filePreviewArea.appendChild(div);
            });
        }
        function addFileTag(name, isLoading, tempId) {
            const div = document.createElement('div');
            div.className = 'file-chip'; div.id = tempId;
            div.innerHTML = `<i class="bi bi-${isLoading ? 'hourglass-split' : 'file-earmark-text'}"></i><span>${name} ${isLoading ? '(ä¸Šä¼ ä¸­...)' : ''}</span>`;
            filePreviewArea.appendChild(div);
        }
        function removeFileTag(id) { const el = document.getElementById(id); if (el) el.remove(); }
        function removeFile(index) { if(!isProcessing) { uploadedFiles.splice(index, 1); renderFileTags(); checkInput(); } }

        async function sendMessage(selectedTool = null, originalQuery = null, toolParams = null, workflowData = null, useHistoryFiles = false) {
            if (isProcessing) return;
            let text = userInput.value.trim();
            if (originalQuery) text = originalQuery;
            
            if (!text && uploadedFiles.length === 0 && !workflowData && !useHistoryFiles) return;

            if (!selectedTool && !workflowData && !useHistoryFiles) {
                appendMessage('user', text || `[å‘é€äº† ${uploadedFiles.length} ä¸ªæ–‡ä»¶]`);
                userInput.value = ''; userInput.style.height = '24px'; 
                checkInput();
            }

            toggleLoadingState(true);
            
            // â±ï¸ å¯åŠ¨è®¡æ—¶å™¨
            const startTime = Date.now();
            const aiMessageDiv = appendMessage('ai', '<span class="spinner-border spinner-border-sm text-primary" role="status"></span><span style="margin-left:8px; color:#666;">æ€è€ƒä¸­...</span>', null, null, true);
            const aiContentDiv = aiMessageDiv; 
            
            // æ‰¾åˆ°åº•éƒ¨æ ï¼Œå‡†å¤‡æ’å…¥è®¡æ—¶å™¨
            const footerDiv = aiMessageDiv.closest('.message-row').querySelector('.message-footer');
            const timerSpan = document.createElement('span');
            timerSpan.className = 'generation-timer';
            timerSpan.innerHTML = '<i class="bi bi-stopwatch"></i> 0.0s';
            footerDiv.prepend(timerSpan); // æ’å…¥åˆ°æœ€å·¦è¾¹

            const timerInterval = setInterval(() => {
                const elapsed = ((Date.now() - startTime) / 1000).toFixed(1);
                timerSpan.innerHTML = `<i class="bi bi-stopwatch"></i> ${elapsed}s`;
            }, 100);

            // åˆå§‹åŒ– AbortController
            abortController = new AbortController();

            const payload = { 
                message: text, 
                history: chatHistory, 
                selected_tool: selectedTool, 
                uploaded_files: uploadedFiles,
                tool_params: toolParams,
                workflow_data: workflowData,
                use_history_files: useHistoryFiles
            };
            
            try {
                const res = await fetch('/api/chat', { 
                    method: 'POST', 
                    headers: { 'Content-Type': 'application/json' }, 
                    body: JSON.stringify(payload),
                    signal: abortController.signal // ç»‘å®šä¿¡å·
                });

                if (!res.ok) throw new Error(await res.text());

                const contentType = res.headers.get("content-type");

                // === åˆ†æ”¯ A: JSON ===
                if (contentType && contentType.includes("application/json")) {
                    const data = await res.json();
                    clearInterval(timerInterval); // åœæ­¢è®¡æ—¶
                    
                    aiMessageDiv.closest('.message-row').remove();

                    // ä¼ é€’ duration ç»™æ¸²æŸ“å‡½æ•°
                    const finalDuration = ((Date.now() - startTime) / 1000).toFixed(1);

                    if (data.type === 'workflow_config') {
                        appendMessage('ai', data.reply, data.thought, finalDuration);
                        renderWorkflowForm(data);
                    }
                    else if (data.type === 'workflow_started') {
                        appendMessage('ai', data.reply, data.thought, finalDuration);
                        startWorkflowPolling(data.run_id);
                    }
                    else if (data.type === 'analysis_report') {
                        renderAnalysisReport(data);
                    }
                    else {
                        appendMessage('ai', marked.parse(data.reply), data.thought, finalDuration, true);
                    }
                    return;
                }

                // === åˆ†æ”¯ B: æµå¼ ===
                aiContentDiv.innerHTML = ''; 
                
                const reader = res.body.getReader();
                const decoder = new TextDecoder();
                let fullText = "";
                
                let thoughtBuffer = "";
                let isThinking = false;
                let hasThoughtBlock = false;
                let finalAnswer = "";

                while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;
                    
                    const chunk = decoder.decode(value, { stream: true });
                    fullText += chunk; 

                    // <think> è§£æé€»è¾‘
                    if (chunk.includes('<think>')) {
                        isThinking = true;
                        hasThoughtBlock = true;
                        if (!aiContentDiv.querySelector('.thought-process')) {
                            aiContentDiv.innerHTML = `
                                <div class="thought-process">
                                    <div class="thought-summary" onclick="toggleThought(this)">
                                        <i class="bi bi-stars"></i> æ·±åº¦æ€è€ƒè¿‡ç¨‹ (ç‚¹å‡»å±•å¼€)
                                    </div>
                                    <div class="thought-content"></div>
                                </div>
                                <div class="final-answer"></div>
                            `;
                        }
                    }

                    if (chunk.includes('</think>')) {
                        isThinking = false;
                        const parts = chunk.split('</think>');
                        thoughtBuffer += parts[0].replace('<think>', '');
                        
                        const thoughtDiv = aiContentDiv.querySelector('.thought-content');
                        if(thoughtDiv) thoughtDiv.innerText = thoughtBuffer;

                        if (parts[1]) {
                            finalAnswer += parts[1];
                            const answerDiv = aiContentDiv.querySelector('.final-answer');
                            if(answerDiv) answerDiv.innerHTML = marked.parse(finalAnswer);
                        }
                        continue; 
                    }

                    if (isThinking) {
                        thoughtBuffer += chunk.replace('<think>', '');
                        const thoughtDiv = aiContentDiv.querySelector('.thought-content');
                        if(thoughtDiv) thoughtDiv.innerText = thoughtBuffer; 
                    } 
                    else {
                        if (!hasThoughtBlock) {
                            finalAnswer += chunk;
                            aiContentDiv.innerHTML = marked.parse(finalAnswer);
                        } else {
                            finalAnswer += chunk;
                            const answerDiv = aiContentDiv.querySelector('.final-answer');
                            if(answerDiv) answerDiv.innerHTML = marked.parse(finalAnswer);
                        }
                    }
                    
                    // ğŸ” ä¸ºå›¾ç‰‡æ·»åŠ ç‚¹å‡»æ”¾å¤§äº‹ä»¶
                    const imgs = aiContentDiv.querySelectorAll('img');
                    imgs.forEach(img => {
                        img.style.cursor = 'zoom-in';
                        img.onclick = () => openLightbox(img.src);
                    });

                    scrollToBottom();
                }
                
                chatHistory.push({ user: text, ai: fullText });

            } catch (error) {
                if (error.name === 'AbortError') {
                    // ç”¨æˆ·æ‰‹åŠ¨åœæ­¢ï¼Œä¸åšå¤„ç†
                } else {
                    aiContentDiv.innerHTML = `<span style="color:red">è¯·æ±‚å¤±è´¥: ${error.message}</span>`;
                }
            } finally { 
                clearInterval(timerInterval); // åœæ­¢è®¡æ—¶
                toggleLoadingState(false); 
                abortController = null;
            }
        }

        // ... (å…¶ä»–æ¸²æŸ“å‡½æ•°ä¿æŒä¸å˜ï¼Œå¦‚ renderAnalysisReport, renderWorkflowForm ç­‰) ...
        // ä¸ºäº†èŠ‚çœç¯‡å¹…ï¼Œè¿™é‡Œçœç•¥äº†æœªä¿®æ”¹çš„è¾…åŠ©å‡½æ•°ï¼Œè¯·ç¡®ä¿ä¿ç•™åŸæœ‰çš„ renderAnalysisReport ç­‰å‡½æ•°
        // âš ï¸ æ³¨æ„ï¼šè¯·ç¡®ä¿ appendMessage å‡½æ•°è¢«æ­£ç¡®æ›´æ–°ï¼ˆè§ä¸‹æ–‡ï¼‰

        function appendMessage(role, content, thoughtContent = null, duration = null, isHtml = false) {
            const row = document.createElement('div');
            row.className = `message-row ${role}`;
            let avatarHtml = role === 'ai' ? `<div class="avatar ai"><i class="bi bi-robot"></i></div>` : '';
            
            // åº•éƒ¨æ  (åŒ…å«è®¡æ—¶å™¨å’Œæ“ä½œæŒ‰é’®)
            let footerHtml = '';
            if (role === 'ai') {
                footerHtml = `
                <div class="message-footer">
                    ${duration ? `<span class="generation-timer"><i class="bi bi-stopwatch"></i> ${duration}s</span>` : ''}
                    <div style="display:flex; gap:10px;">
                        <i class="bi bi-copy" style="cursor:pointer" onclick="copyText(this)" title="å¤åˆ¶å›ç­”"></i>
                    </div>
                </div>`;
            }
            
            let userActionsHtml = '';
            if (role === 'user') {
                userActionsHtml = `
                <div class="user-actions">
                    <div class="action-item" onclick="editMessage(this)"><i class="bi bi-pencil"></i> ç¼–è¾‘</div>
                    <div class="action-item" onclick="copyText(this)"><i class="bi bi-copy"></i> å¤åˆ¶</div>
                </div>`;
            }

            const bubbleContent = isHtml ? content : marked.parse(content);
            row.innerHTML = `${role === 'ai' ? avatarHtml : ''}<div class="message-container"><div class="bubble">${bubbleContent}</div>${userActionsHtml}${footerHtml}</div>`;
            chatContent.appendChild(row);
            scrollToBottom();
            return row.querySelector('.bubble');
        }

        // ... (ä¿ç•™åŸæœ‰çš„ renderWorkflowForm, startWorkflowPolling, renderAnalysisReport ç­‰å‡½æ•°) ...
        // âš ï¸ è¯·åŠ¡å¿…å°†ä¹‹å‰å®Œæ•´çš„è¾…åŠ©å‡½æ•°ç²˜è´´å›æ¥ï¼Œæˆ–è€…åªæ›¿æ¢ä¸Šé¢çš„ sendMessage å’Œ appendMessage
        
        // ä¸ºäº†æ–¹ä¾¿ä½ ç›´æ¥å¤åˆ¶ï¼Œæˆ‘æŠŠæ‰€æœ‰è¾…åŠ©å‡½æ•°å†è´´ä¸€éï¼š
        
        function renderAnalysisReport(data) {
            const reportId = `report-${Date.now()}`;
            const reportData = data.report_data;
            const diagnosis = data.diagnosis;
            
            let html = `
            <div class="analysis-report" id="${reportId}">
                <div class="report-header">
                    <span class="report-title"><i class="bi bi-file-earmark-medical"></i> å•ç»†èƒåˆ†ææŠ¥å‘Š</span>
                    <button class="btn btn-outline-secondary btn-sm" onclick="window.print()">
                        <i class="bi bi-printer"></i> æ‰“å°/ä¿å­˜PDF
                    </button>
                </div>
                <div class="report-section"><h6>1. æ‰§è¡Œæµç¨‹è¯¦æƒ…</h6><div class="accordion" id="acc-${reportId}">`;
            
            reportData.steps_details.forEach((step, index) => {
                const stepId = `step-${reportId}-${index}`;
                const statusIcon = step.status === 'success' ? 'âœ…' : 'âŒ';
                const plotHtml = step.plot ? `<div class="mt-2"><img src="${step.plot}" style="max-width:100%; border-radius:4px; cursor:zoom-in;" onclick="openLightbox(this.src)"></div>` : '';
                html += `<div class="accordion-item"><h2 class="accordion-header"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#${stepId}">${statusIcon} ${step.name} <span class="ms-auto text-muted small">${step.summary}</span></button></h2><div id="${stepId}" class="accordion-collapse collapse" data-bs-parent="#acc-${reportId}"><div class="accordion-body"><div class="small text-secondary mb-2">${step.details}</div>${plotHtml}</div></div></div>`;
            });
            
            html += `</div></div><div class="report-section"><h6>2. AI ä¸“å®¶è¯Šæ–­ä¸å»ºè®®</h6><div class="llm-diagnosis">${marked.parse(diagnosis)}</div></div><div class="report-section"><h6>3. æœ€ç»ˆå¯è§†åŒ–ç»“æœ</h6>${reportData.final_plot ? `<img src="${reportData.final_plot}" style="width:100%; border-radius:8px; cursor:zoom-in;" onclick="openLightbox(this.src)">` : 'æ— å›¾è¡¨ç”Ÿæˆ'}</div></div>`;
            appendMessage('ai', html, null, null, true);
        }

        function renderDataSelector(datasets) {
            const selectorId = `ds-${Date.now()}`;
            let html = `<div class="data-selector-container" id="${selectorId}"><div class="data-selector-header"><i class="bi bi-folder2-open"></i> é€‰æ‹© Galaxy å†å²æ•°æ®</div><div class="data-selector-list">`;
            datasets.forEach(d => {
                html += `<div class="dataset-item" onclick="toggleCheckbox(this)"><input type="checkbox" class="form-check-input" value="${d.id}" data-name="${d.name}" onclick="event.stopPropagation(); toggleCheckbox(this.parentElement, true)"><div class="dataset-info"><span class="dataset-name">${d.hid}: ${d.name}</span><span class="dataset-meta">${d.ext} | ${d.size}</span></div></div>`;
            });
            html += `</div><div class="data-selector-footer"><button class="btn btn-primary btn-sm" onclick="submitDataSelection('${selectorId}')">ç¡®è®¤ä½¿ç”¨ <i class="bi bi-check2"></i></button></div></div>`;
            appendMessage('ai', html, null, null, true);
        }

        function toggleCheckbox(row, fromCheckbox = false) {
            const checkbox = row.querySelector('input[type="checkbox"]');
            if (!fromCheckbox) checkbox.checked = !checkbox.checked;
            if(checkbox.checked) row.classList.add('selected'); else row.classList.remove('selected');
        }

        function submitDataSelection(selectorId) {
            const container = document.getElementById(selectorId);
            const checkboxes = container.querySelectorAll('input:checked');
            if (checkboxes.length === 0) { alert("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶"); return; }
            uploadedFiles = []; let selectedNames = [];
            checkboxes.forEach(cb => { const name = cb.getAttribute('data-name'); uploadedFiles.push({ id: cb.value, name: name }); selectedNames.push(name); });
            const btn = container.querySelector('button'); btn.disabled = true; btn.innerText = "å·²é€‰æ‹©";
            const userMsg = `æˆ‘å·²é€‰æ‹©ä»¥ä¸‹æ–‡ä»¶ï¼š\n${selectedNames.join('\n')}\n\nè¯·åŸºäºè¿™äº›æ–‡ä»¶ï¼Œç»§ç»­è§„åˆ’ Seurat å•ç»†èƒåˆ†æå·¥ä½œæµã€‚`;
            appendMessage('user', userMsg);
            sendMessage(null, "å·²é€‰æ‹©æ–‡ä»¶ï¼Œè¯·ç»§ç»­è§„åˆ’ Seurat å•ç»†èƒå·¥ä½œæµ", null, null, true);
        }

        function startWorkflowPolling(runId) {
            const monitorId = `monitor-${runId}`;
            const html = `<div class="workflow-monitor" id="${monitorId}"><h6>ğŸš€ å·¥ä½œæµæ‰§è¡Œè¿›åº¦</h6><div class="steps-container"></div></div>`;
            appendMessage('ai', html, null, null, true);
            const container = document.getElementById(monitorId).querySelector('.steps-container');
            const poll = setInterval(async () => {
                try {
                    const res = await fetch(`/api/workflow/status/${runId}`);
                    const data = await res.json();
                    if (data.status === 'not_found') { clearInterval(poll); return; }
                    let stepsHtml = '';
                    data.steps_status.forEach((step, index) => {
                        let iconClass = 'pending'; let iconContent = '<i class="bi bi-circle"></i>';
                        if (step.status === 'running') { iconClass = 'running'; iconContent = '<i class="bi bi-arrow-repeat"></i>'; }
                        else if (step.status === 'success') { iconClass = 'success'; iconContent = '<i class="bi bi-check-lg"></i>'; }
                        else if (step.status === 'failed') { iconClass = 'failed'; iconContent = '<i class="bi bi-x-lg"></i>'; }
                        stepsHtml += `<div class="step-item"><div class="step-icon ${iconClass}">${iconContent}</div><div class="step-name">${step.name || 'Step ' + (index + 1)}</div><div class="step-status-text">${step.status}</div></div>`;
                    });
                    container.innerHTML = stepsHtml;
                    if (data.completed) {
                        clearInterval(poll);
                        if (data.status === 'success') {
                            container.innerHTML += `<div class="alert alert-success mt-3 mb-0">ğŸ‰ å·¥ä½œæµå…¨éƒ¨æ‰§è¡Œå®Œæˆï¼æ­£åœ¨åŠ è½½æŠ¥å‘Š...</div>`;
                            const reportPayload = { diagnosis: "âœ… **åˆ†ææˆåŠŸï¼**\n\næ•°æ®å·²å®Œæˆå…¨æµç¨‹å¤„ç†ï¼Œä¸‹æ–¹å±•ç¤ºäº†æœ€ç»ˆçš„ UMAP é™ç»´å¯è§†åŒ–ç»“æœåŠå…³é”®è´¨æ§æŒ‡æ ‡ã€‚", report_data: data.report_data };
                            setTimeout(() => { renderAnalysisReport(reportPayload); scrollToBottom(); }, 500);
                        } else { container.innerHTML += `<div class="alert alert-danger mt-3 mb-0">âŒ æ‰§è¡Œå‡ºé”™: ${data.error}</div>`; }
                    }
                } catch (e) { console.error(e); }
            }, 2000);
        }

        function renderWorkflowForm(data) {
            const formId = `wf-${Date.now()}`;
            let formHtml = `<div class="params-form" id="${formId}"><h5>ğŸ“‹ ${data.workflow_name}</h5><p class="text-muted">è¯·ç¡®è®¤æˆ–ä¿®æ”¹æ¯ä¸€æ­¥çš„å‚æ•°ï¼š</p><div class="accordion" id="acc-${formId}">`;
            data.steps.forEach((step, index) => {
                const stepId = `step-${index}`;
                formHtml += `<div class="accordion-item"><h2 class="accordion-header d-flex align-items-center"><div class="form-check ms-3 me-2"><input class="form-check-input step-checkbox" type="checkbox" checked data-index="${index}"></div><button class="accordion-button ${index > 0 ? 'collapsed' : ''}" type="button" data-bs-toggle="collapse" data-bs-target="#${stepId}">${step.name}</button></h2><div id="${stepId}" class="accordion-collapse collapse ${index === 0 ? 'show' : ''}" data-bs-parent="#acc-${formId}"><div class="accordion-body"><p class="text-muted small">${step.desc}</p><input type="hidden" name="tool_id_${index}" value="${step.tool_id}"><input type="hidden" name="step_name_${index}" value="${step.name}">`;
                step.params.forEach(p => {
                    const inputName = `param_${index}_${p.name}`;
                    formHtml += `<div class="mb-2"><label class="form-label small">${p.label}</label>`;
                    if (p.type === 'select') {
                        formHtml += `<select class="form-select form-select-sm" name="${inputName}">`;
                        p.options.forEach(opt => { const selected = opt.value === p.value ? 'selected' : ''; formHtml += `<option value="${opt.value}" ${selected}>${opt.label}</option>`; });
                        formHtml += `</select>`;
                    } else if (p.type === 'boolean') {
                        const checked = p.value === 'true' ? 'checked' : '';
                        formHtml += `<div class="form-check form-switch"><input class="form-check-input" type="checkbox" name="${inputName}" ${checked}></div>`;
                    } else { formHtml += `<input type="text" class="form-control form-control-sm" name="${inputName}" value="${p.value || ''}">`; }
                    formHtml += `</div>`;
                });
                formHtml += `</div></div></div>`;
            });
            formHtml += `</div><button class="btn btn-success btn-run mt-3" onclick="submitWorkflow('${formId}', ${data.steps.length})">ğŸš€ æ‰§è¡Œå·¥ä½œæµ</button></div>`;
            appendMessage('ai', formHtml, null, null, true);
        }

        function submitWorkflow(formId, totalSteps) {
            if (uploadedFiles.length === 0) { alert("âš ï¸ è¯·å…ˆä¸Šä¼ å•ç»†èƒæ•°æ®æ–‡ä»¶ (.h5ad, .mtx ç­‰) å†æ‰§è¡Œå·¥ä½œæµï¼"); document.getElementById('fileInput').click(); return; }
            const formContainer = document.getElementById(formId);
            const stepsData = [];
            const checkboxes = formContainer.querySelectorAll('.step-checkbox');
            checkboxes.forEach((cb, i) => {
                if (cb.checked) {
                    const toolId = formContainer.querySelector(`input[name="tool_id_${i}"]`).value;
                    const stepName = formContainer.querySelector(`input[name="step_name_${i}"]`).value;
                    const params = {};
                    const inputs = formContainer.querySelectorAll(`[name^="param_${i}_"]`);
                    inputs.forEach(input => {
                        const originalName = input.name.replace(`param_${i}_`, '');
                        if (input.type === 'checkbox') params[originalName] = input.checked;
                        else params[originalName] = input.value;
                    });
                    stepsData.push({ name: stepName, tool_id: toolId, params: params });
                }
            });
            if (stepsData.length === 0) { alert("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ­¥éª¤ï¼"); return; }
            const btn = formContainer.querySelector('button'); btn.disabled = true; btn.innerText = "å·¥ä½œæµæ‰§è¡Œä¸­...";
            sendMessage(null, "Execute Workflow", null, { steps: stepsData });
        }

        function renderToolForm(data) {
            const formId = `form-${Date.now()}`;
            let formHtml = `<div class="params-form" id="${formId}"><h5>ğŸ› ï¸ ${data.tool_name} å‚æ•°é…ç½®</h5><hr>`;
            data.params.forEach(p => {
                formHtml += `<div class="mb-3"><label class="form-label">${p.label}</label>`;
                if (p.type === 'select') {
                    formHtml += `<select class="form-select" name="${p.name}">`;
                    p.options.forEach(opt => { const selected = opt.value === p.value ? 'selected' : ''; formHtml += `<option value="${opt.value}" ${selected}>${opt.label}</option>`; });
                    formHtml += `</select>`;
                } else if (p.type === 'boolean') {
                    const checked = p.value === 'true' ? 'checked' : '';
                    formHtml += `<div class="form-check form-switch"><input class="form-check-input" type="checkbox" name="${p.name}" ${checked}></div>`;
                } else { formHtml += `<input type="text" class="form-control" name="${p.name}" value="${p.value || ''}">`; }
                if (p.help) formHtml += `<div class="form-text">${p.help}</div>`;
                formHtml += `</div>`;
            });
            const toolInfoStr = JSON.stringify({name: data.tool_name, id: data.tool_id}).replace(/"/g, '&quot;');
            formHtml += `<button class="btn btn-primary btn-run" onclick="submitToolForm('${formId}', ${toolInfoStr})">ğŸš€ ç¡®è®¤å¹¶æ‰§è¡Œ</button></div>`;
            appendMessage('ai', formHtml, null, null, true);
        }

        function submitToolForm(formId, toolInfo) {
            const formContainer = document.getElementById(formId);
            const inputs = formContainer.querySelectorAll('input, select');
            const params = {};
            inputs.forEach(input => { if (input.type === 'checkbox') params[input.name] = input.checked; else params[input.name] = input.value; });
            const btn = formContainer.querySelector('button'); btn.disabled = true; btn.innerText = "å·²æäº¤...";
            sendMessage(toolInfo, `Run ${toolInfo.name} with params`, params);
        }

        function formatExecutionResult(text) {
            text = text.trim();
            let tableHtml = '<div class="props-panel">';
            let hasData = false;
            if (text.startsWith('{') && text.endsWith('}')) {
                try {
                    let jsonStr = text.replace(/'/g, '"').replace(/\bFalse\b/g, 'false').replace(/\bTrue\b/g, 'true').replace(/\bNone\b/g, 'null');
                    const obj = JSON.parse(jsonStr);
                    for (const [key, value] of Object.entries(obj)) {
                        hasData = true;
                        const displayValue = (typeof value === 'object' && value !== null) ? JSON.stringify(value) : value;
                        tableHtml += `<div class="prop-row"><div class="prop-key">${key}</div><div class="prop-value">${displayValue}</div></div>`;
                    }
                } catch (e) { console.warn("Dict parse failed"); }
            }
            if (!hasData) {
                const lines = text.split('\n');
                lines.forEach(line => {
                    const match = line.match(/^\s*([^:]+):\s*(.*)$/);
                    if (match) {
                        hasData = true;
                        tableHtml += `<div class="prop-row"><div class="prop-key">${match[1].trim()}</div><div class="prop-value">${match[2].trim()}</div></div>`;
                    }
                });
            }
            tableHtml += '</div>';
            return hasData ? `<h6>âœ… æ‰§è¡Œç»“æœ</h6>${tableHtml}` : `<h6>âœ… æ‰§è¡Œç»“æœ</h6><pre>${text}</pre>`;
        }

        function toggleCode(btn) {
            const container = btn.nextElementSibling;
            if (container.style.display === 'block') { container.style.display = 'none'; btn.innerHTML = '<i class="bi bi-code-slash"></i> æŸ¥çœ‹ç­–ç•¥ä»£ç '; }
            else { container.style.display = 'block'; btn.innerHTML = '<i class="bi bi-chevron-up"></i> æ”¶èµ·ä»£ç '; }
        }

        function toggleThought(header) {
            const content = header.nextElementSibling;
            const icon = header.querySelector('.bi-chevron-down');
            if (content.classList.contains('open')) {
                content.classList.remove('open');
                header.querySelector('i').className = 'bi bi-stars'; 
            } else {
                content.classList.add('open');
                header.querySelector('i').className = 'bi bi-chevron-down'; 
            }
        }

        function copyText(btn) {
            let text = "";
            const container = btn.closest('.message-container');
            if (container) text = container.querySelector('.bubble').innerText;
            if (navigator.clipboard && window.isSecureContext) {
                navigator.clipboard.writeText(text).then(() => {
                    const icon = btn.querySelector('i');
                    if(icon) { const originalClass = icon.className; icon.className = 'bi bi-check'; setTimeout(() => icon.className = originalClass, 2000); }
                });
            } else {
                const textArea = document.createElement("textarea");
                textArea.value = text;
                textArea.style.position = "fixed"; textArea.style.left = "-9999px";
                document.body.appendChild(textArea); textArea.focus(); textArea.select();
                try { document.execCommand('copy'); } catch (err) {}
                document.body.removeChild(textArea);
            }
        }

        function editMessage(btn) {
            const container = btn.closest('.message-container');
            const text = container.querySelector('.bubble').innerText;
            userInput.value = text; userInput.focus(); autoResize(userInput); checkInput();
        }

        function selectTool(tool, query) { appendMessage('user', `æˆ‘é€‰æ‹©è¿è¡Œ: <b>${tool.name}</b>`); sendMessage(tool, query); }
        function scrollToBottom() { chatScrollArea.scrollTop = chatScrollArea.scrollHeight; }
    </script>
</body>
</html>

--- END OF FILE: services/nginx/html/index.html ---

--- START OF FILE: services/nginx/conf.d/default.conf ---
upstream backend_api {
    server api-server:8000;
}

server {
    listen 80;
    server_name localhost;

    # 1. å‰ç«¯é™æ€é¡µé¢
    location / {
        root /usr/share/nginx/html;
        index index.html;
        try_files $uri $uri/ /index.html;
    }

    # 2. API æ¥å£è½¬å‘
    location /api/ {
        proxy_pass http://backend_api;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        client_max_body_size 10G;
        proxy_read_timeout 300s;
        proxy_connect_timeout 75s;

        # âœ… æµå¼ä¼ è¾“ä¼˜åŒ– (ä¿®æ­£ç‰ˆ)
        proxy_http_version 1.1;      # å¯ç”¨ HTTP/1.1 æ”¯æŒé•¿è¿æ¥
        proxy_set_header Connection ""; # æ¸…ç©º Connection å¤´ï¼Œé˜²æ­¢ç”± HTTP/1.0 å…³é—­è¿æ¥
        proxy_buffering off;         # å…³é—­ç¼“å†²ï¼Œè®©æ•°æ®å®æ—¶æµå‡º
        proxy_cache off;             # å…³é—­ç¼“å­˜
        tcp_nodelay on;              # ç¦ç”¨ Nagle ç®—æ³•
    }

    # 3. é™æ€èµ„æºæœåŠ¡ (å›¾ç‰‡)
    location /uploads/ {
        alias /app/uploads/;
        autoindex on;
        expires 1d;
    }
}

--- END OF FILE: services/nginx/conf.d/default.conf ---

